#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Demo error handling vá»›i timeout tháº¥p Ä‘á»ƒ táº¡o lá»—i
"""

from sap_nhap_simple import SapNhapCrawlerSimple
import time

def main():
    print("=== DEMO ERROR HANDLING ===")
    print("ğŸŒ Website: https://thuvienphapluat.vn")
    print("ğŸ¯ Demo vá»›i timeout tháº¥p Ä‘á»ƒ táº¡o lá»—i")
    print("=" * 50)
    
    crawler = SapNhapCrawlerSimple()
    
    # Hack Ä‘á»ƒ táº¡o lá»—i: giáº£m timeout xuá»‘ng ráº¥t tháº¥p
    original_get_page_content = crawler.get_page_content
    
    def get_page_content_with_errors(url, ma_tinh=None, ma_xa=None, ten_tinh=None, ten_xa=None):
        """Version vá»›i timeout tháº¥p Ä‘á»ƒ demo lá»—i"""
        # Táº¡o lá»—i timeout cho má»™t sá»‘ request
        import random
        if random.random() < 0.3:  # 30% chance táº¡o lá»—i
            print(f"    ğŸ”§ Demo: Táº¡o lá»—i timeout cho {url}")
            crawler.log_error('timeout', url, 'Demo timeout error', ma_tinh, ma_xa, ten_tinh, ten_xa)
            return None
        
        return original_get_page_content(url, ma_tinh, ma_xa, ten_tinh, ten_xa)
    
    # Thay tháº¿ method Ä‘á»ƒ demo
    crawler.get_page_content = get_page_content_with_errors
    
    try:
        # Test vá»›i HÃ  Ná»™i - chá»‰ 10 xÃ£/phÆ°á»ng Ä‘á»ƒ demo
        provinces = crawler.get_provinces_from_html()
        if provinces:
            ha_noi = provinces[0]  # HÃ  Ná»™i
            ma_tinh = ha_noi['ma_tinh']
            ten_tinh = ha_noi['ten_tinh']
            
            print(f"\nğŸ“ Demo vá»›i {ten_tinh} (MÃ£: {ma_tinh})")
            
            xa_phuong_list = crawler.get_xa_phuong_from_province(ma_tinh, ten_tinh)
            
            if xa_phuong_list:
                # Chá»‰ láº¥y 10 xÃ£/phÆ°á»ng Ä‘áº§u tiÃªn Ä‘á»ƒ demo
                demo_list = xa_phuong_list[:10]
                print(f"ğŸ“Š Demo vá»›i {len(demo_list)} xÃ£/phÆ°á»ng Ä‘áº§u tiÃªn")
                
                for i, xa in enumerate(demo_list, 1):
                    ma_xa = xa['ma_xa']
                    ten_xa = xa['ten_xa']
                    
                    print(f"\n  [{i}/{len(demo_list)}] {ten_xa}")
                    
                    xa_info = crawler.get_sap_nhap_details(ma_tinh, ma_xa, ten_tinh, ten_xa)
                    
                    if xa_info:
                        crawler.data.append(xa_info)
                    
                    time.sleep(0.5)  # Delay ngáº¯n cho demo
        
        print(f"\nğŸ“Š === Káº¾T QUáº¢ DEMO ===")
        print(f"ğŸ“ˆ Dá»¯ liá»‡u thÃ nh cÃ´ng: {len(crawler.data)}")
        print(f"âŒ Sá»‘ lá»—i táº¡o ra: {len(crawler.error_log)}")
        
        # LÆ°u dá»¯ liá»‡u vá»›i thá»‘ng kÃª lá»—i
        if crawler.data or crawler.error_log:
            filename = crawler.save_to_excel()
            print(f"ğŸ’¾ File káº¿t quáº£: {filename}")
        
        # Demo retry náº¿u cÃ³ lá»—i
        if crawler.error_log:
            print(f"\nğŸ”„ === DEMO RETRY ===")
            print(f"CÃ³ {len(crawler.error_log)} lá»—i, thá»­ retry...")
            
            # KhÃ´i phá»¥c method gá»‘c Ä‘á»ƒ retry thÃ nh cÃ´ng
            crawler.get_page_content = original_get_page_content
            
            retry_data = crawler.retry_failed_requests()
            
            if retry_data:
                print(f"âœ… Retry thÃ nh cÃ´ng {len(retry_data)} báº£n ghi")
                # Merge vá»›i dá»¯ liá»‡u chÃ­nh
                all_data = crawler.data + retry_data
                crawler.data = all_data
                
                # LÆ°u file cuá»‘i cÃ¹ng
                final_filename = crawler.save_to_excel()
                print(f"ğŸ’¾ File cuá»‘i cÃ¹ng: {final_filename}")
        
        print(f"\nğŸ‰ DEMO HOÃ€N THÃ€NH!")
            
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ÄÃ£ dá»«ng theo yÃªu cáº§u ngÆ°á»i dÃ¹ng")
        if crawler.data or crawler.error_log:
            print("ğŸ’¾ Äang lÆ°u dá»¯ liá»‡u Ä‘Ã£ kÃ©o Ä‘Æ°á»£c...")
            crawler.save_to_excel()

if __name__ == "__main__":
    main()
