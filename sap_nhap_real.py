#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script thá»±c táº¿ Ä‘á»ƒ kÃ©o dá»¯ liá»‡u - sá»­ dá»¥ng cÃ¡ch tiáº¿p cáº­n thá»±c táº¿
"""

import requests
import json
import pandas as pd
import time
from bs4 import BeautifulSoup
from urllib.parse import parse_qs, urlparse
import re
import os
from datetime import datetime
import html

class SapNhapCrawlerReal:
    def __init__(self):
        self.base_url = "https://thuvienphapluat.vn"
        self.search_url = "https://thuvienphapluat.vn/ma-so-thue/tra-cuu-thong-tin-sap-nhap-tinh"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.data = []
        self.failed_requests = []
        
        # Danh sÃ¡ch tá»‰nh/thÃ nh cá»‘ Ä‘á»‹nh
        self.provinces = [
            {'ma_tinh': '01', 'ten_tinh': 'HÃ  Ná»™i'},
            {'ma_tinh': '79', 'ten_tinh': 'Há»“ ChÃ­ Minh'}, 
            {'ma_tinh': '83', 'ten_tinh': 'VÄ©nh Long'},  # ÄÃ£ test cÃ³ dá»¯ liá»‡u
            {'ma_tinh': '89', 'ten_tinh': 'An Giang'},
            {'ma_tinh': '92', 'ten_tinh': 'Cáº§n ThÆ¡'},
        ]
        
    def get_page_content(self, url):
        """Láº¥y ná»™i dung trang web"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except requests.RequestException as e:
            self.failed_requests.append({'url': url, 'error': str(e)})
            return None
    
    def discover_xa_phuong_from_known_page(self, ma_tinh):
        """KhÃ¡m phÃ¡ danh sÃ¡ch xÃ£/phÆ°á»ng tá»« trang Ä‘Ã£ biáº¿t cÃ³ dá»¯ liá»‡u"""
        # Danh sÃ¡ch cÃ¡c mÃ£ xÃ£/phÆ°á»ng máº«u Ä‘á»ƒ thá»­
        sample_xa_codes = ['28996', '28756', '28757', '1', '100', '200']
        
        xa_phuong_list = []
        found_data = False
        
        for sample_xa in sample_xa_codes:
            if found_data:
                break
                
            sample_url = f"{self.search_url}?MaTinh={ma_tinh}&MaXa={sample_xa}"
            content = self.get_page_content(sample_url)
            
            if not content:
                continue
            
            soup = BeautifulSoup(content, 'html.parser')
            
            # TÃ¬m táº¥t cáº£ link cÃ³ MaXa
            xa_links = soup.find_all('a', href=re.compile(r'MaXa=\d+'))
            
            if xa_links:
                print(f"    ğŸ” TÃ¬m tháº¥y {len(xa_links)} link tá»« máº«u MaXa={sample_xa}")
                found_data = True
                
                seen_xa = set()
                for link in xa_links:
                    href = link.get('href', '')
                    if f'MaTinh={ma_tinh}' in href:
                        parsed = urlparse(href)
                        query_params = parse_qs(parsed.query)
                        if 'MaXa' in query_params:
                            ma_xa = query_params['MaXa'][0]
                            if ma_xa not in seen_xa:
                                seen_xa.add(ma_xa)
                                text = link.text.strip()
                                text = re.sub(r'^\d+\.\s*', '', text)
                                xa_phuong_list.append({
                                    'ma_xa': ma_xa,
                                    'ten_xa': text
                                })
                break
        
        if not found_data:
            print(f"    âš ï¸  KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u xÃ£/phÆ°á»ng tá»« cÃ¡c máº«u")
        
    def discover_xa_phuong_by_scanning(self, ma_tinh, max_scan=50):
        """QuÃ©t theo range Ä‘á»ƒ tÃ¬m mÃ£ xÃ£/phÆ°á»ng há»£p lá»‡"""
        print(f"    ğŸ” QuÃ©t tÃ¬m mÃ£ xÃ£ há»£p lá»‡ cho tá»‰nh {ma_tinh}...")
        
        valid_xa = []
        scan_ranges = [
            range(1, 100),           # MÃ£ nhá»
            range(1000, 1100),       # MÃ£ trung bÃ¬nh
            range(28000, 29000),     # MÃ£ theo pattern Ä‘Ã£ tháº¥y
            range(10000, 10100),     # MÃ£ khÃ¡c
        ]
        
        found_count = 0
        
        for scan_range in scan_ranges:
            if found_count >= max_scan:
                break
                
            for ma_xa in scan_range:
                if found_count >= max_scan:
                    break
                    
                test_url = f"{self.search_url}?MaTinh={ma_tinh}&MaXa={ma_xa}"
                content = self.get_page_content(test_url)
                
                if content:
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # Kiá»ƒm tra xem cÃ³ thÃ´ng tin thá»±c sá»± khÃ´ng
                    text = soup.get_text().lower()
                    if any(keyword in text for keyword in ['sÃ¡p nháº­p', 'phÆ°á»ng', 'xÃ£', 'thá»‹ tráº¥n']):
                        # CÃ³ váº» nhÆ° cÃ³ dá»¯ liá»‡u thá»±c
                        title_tag = soup.find('title')
                        if title_tag:
                            title = title_tag.text.strip()
                            if 'tá»‰nh' not in title.lower():  # KhÃ´ng pháº£i trang tá»•ng quÃ¡t
                                valid_xa.append({
                                    'ma_xa': str(ma_xa),
                                    'ten_xa': f'ÄÆ¡n vá»‹ hÃ nh chÃ­nh {ma_xa}',
                                    'url': test_url
                                })
                                found_count += 1
                                print(f"      âœ“ TÃ¬m tháº¥y MaXa={ma_xa}")
                
                time.sleep(0.1)  # Delay nhá»
        
        print(f"    ğŸ“Š TÃ¬m Ä‘Æ°á»£c {found_count} mÃ£ xÃ£ há»£p lá»‡")
        return valid_xa
    
    def parse_sap_nhap_info(self, soup):
        """PhÃ¢n tÃ­ch thÃ´ng tin sÃ¡p nháº­p"""
        info = {
            'truoc_sap_nhap': '',
            'sau_sap_nhap': '',
            'chi_tiet': []
        }
        
        # TÃ¬m báº£ng cÃ³ thÃ´ng tin sÃ¡p nháº­p
        tables = soup.find_all('table')
        
        for table in tables:
            rows = table.find_all('tr')
            
            for i, row in enumerate(rows):
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    cell_texts = [cell.get_text(strip=True).lower() for cell in cells]
                    
                    has_truoc = any('trÆ°á»›c sÃ¡p nháº­p' in text for text in cell_texts)
                    has_sau = any('sau sÃ¡p nháº­p' in text for text in cell_texts)
                    
                    if has_truoc and has_sau:
                        truoc_col = next((j for j, text in enumerate(cell_texts) if 'trÆ°á»›c sÃ¡p nháº­p' in text), 0)
                        sau_col = next((j for j, text in enumerate(cell_texts) if 'sau sÃ¡p nháº­p' in text), 1)
                        
                        for j in range(i + 1, len(rows)):
                            data_row = rows[j]
                            data_cells = data_row.find_all(['td', 'th'])
                            
                            if len(data_cells) > max(truoc_col, sau_col):
                                truoc_text = html.unescape(data_cells[truoc_col].get_text(strip=True))
                                sau_text = html.unescape(data_cells[sau_col].get_text(strip=True))
                                
                                if truoc_text and sau_text:
                                    info['chi_tiet'].append({
                                        'truoc': truoc_text,
                                        'sau': sau_text
                                    })
                                    
                                    if not info['truoc_sap_nhap']:
                                        info['truoc_sap_nhap'] = truoc_text
                                    if not info['sau_sap_nhap']:
                                        info['sau_sap_nhap'] = sau_text
                        break
        
        return info
    
    def get_sap_nhap_details(self, ma_tinh, ma_xa=None, ten_tinh='', ten_xa=''):
        """Láº¥y chi tiáº¿t thÃ´ng tin sÃ¡p nháº­p"""
        if ma_xa:
            url = f"{self.search_url}?MaTinh={ma_tinh}&MaXa={ma_xa}"
            cap_hanh_chinh = 'XÃ£/PhÆ°á»ng'
        else:
            url = f"{self.search_url}?MaTinh={ma_tinh}"
            cap_hanh_chinh = 'Tá»‰nh/ThÃ nh phá»‘'
        
        content = self.get_page_content(url)
        if not content:
            return None
        
        soup = BeautifulSoup(content, 'html.parser')
        sap_nhap_info = self.parse_sap_nhap_info(soup)
        
        result = {
            'ma_tinh': ma_tinh,
            'ten_tinh': ten_tinh,
            'ma_xa': ma_xa or '',
            'ten_xa': ten_xa,
            'cap_hanh_chinh': cap_hanh_chinh,
            'url': url,
            'truoc_sap_nhap': sap_nhap_info['truoc_sap_nhap'],
            'sau_sap_nhap': sap_nhap_info['sau_sap_nhap'],
            'chi_tiet_json': json.dumps(sap_nhap_info['chi_tiet'], ensure_ascii=False) if sap_nhap_info['chi_tiet'] else '',
            'so_luong_thay_doi': len(sap_nhap_info['chi_tiet'])
        }
        
        return result
    
    def crawl_province_data(self, province, max_xa=None):
        """KÃ©o dá»¯ liá»‡u cho má»™t tá»‰nh"""
        ma_tinh = province['ma_tinh']
        ten_tinh = province['ten_tinh']
        
        print(f"\n--- Xá»­ lÃ½ {ten_tinh} (MÃ£: {ma_tinh}) ---")
        
        province_data = []
        
        # Láº¥y thÃ´ng tin cáº¥p tá»‰nh
        tinh_info = self.get_sap_nhap_details(ma_tinh, ten_tinh=ten_tinh)
        if tinh_info:
            province_data.append(tinh_info)
            print(f"âœ“ Láº¥y Ä‘Æ°á»£c thÃ´ng tin tá»‰nh")
        
        # KhÃ¡m phÃ¡ danh sÃ¡ch xÃ£/phÆ°á»ng
        xa_phuong_list = self.discover_xa_phuong_from_known_page(ma_tinh)
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y báº±ng cÃ¡ch 1, thá»­ quÃ©t
        if not xa_phuong_list:
            print(f"    ğŸ” KhÃ´ng tÃ¬m tháº¥y tá»« máº«u, thá»­ quÃ©t...")
            xa_phuong_list = self.discover_xa_phuong_by_scanning(ma_tinh, max_scan=20)
        
        print(f"ğŸ” TÃ¬m tháº¥y {len(xa_phuong_list)} xÃ£/phÆ°á»ng")
        
        # Giá»›i háº¡n sá»‘ lÆ°á»£ng Ä‘á»ƒ test
        if max_xa and len(xa_phuong_list) > max_xa:
            xa_phuong_list = xa_phuong_list[:max_xa]
            print(f"ğŸ“Š Giá»›i háº¡n {max_xa} xÃ£ Ä‘áº§u tiÃªn Ä‘á»ƒ test")
        
        # Xá»­ lÃ½ tá»«ng xÃ£/phÆ°á»ng
        success_count = 0
        for i, xa_phuong in enumerate(xa_phuong_list):
            ma_xa = xa_phuong['ma_xa']
            ten_xa = xa_phuong['ten_xa']
            
            print(f"  ğŸ“ [{i+1}/{len(xa_phuong_list)}] {ten_xa} (MÃ£: {ma_xa})")
            
            xa_info = self.get_sap_nhap_details(ma_tinh, ma_xa, ten_tinh, ten_xa)
            if xa_info:
                province_data.append(xa_info)
                success_count += 1
                
                # Hiá»ƒn thá»‹ thÃ´ng tin náº¿u cÃ³
                if xa_info['truoc_sap_nhap'] or xa_info['sau_sap_nhap']:
                    print(f"    âœ“ CÃ³ thÃ´ng tin sÃ¡p nháº­p")
            
            time.sleep(0.3)  # Delay
            
            # Status update má»—i 10 item
            if (i + 1) % 10 == 0:
                print(f"    ğŸ“ˆ ÄÃ£ xá»­ lÃ½ {i+1}/{len(xa_phuong_list)}")
        
        print(f"âœ… HoÃ n thÃ nh {ten_tinh}: {len(province_data)} báº£n ghi ({success_count} xÃ£/phÆ°á»ng)")
        return province_data
    
    def crawl_sample_data(self, max_provinces=2, max_xa_per_province=10):
        """KÃ©o dá»¯ liá»‡u máº«u"""
        print(f"ğŸš€ Báº¯t Ä‘áº§u kÃ©o dá»¯ liá»‡u máº«u: {max_provinces} tá»‰nh, tá»‘i Ä‘a {max_xa_per_province} xÃ£/tá»‰nh")
        
        sample_provinces = self.provinces[:max_provinces]
        
        for province in sample_provinces:
            province_data = self.crawl_province_data(province, max_xa_per_province)
            self.data.extend(province_data)
        
        print(f"ğŸ‰ HoÃ n thÃ nh! Tá»•ng cá»™ng: {len(self.data)} báº£n ghi")
        return self.data
    
    def crawl_full_data(self):
        """KÃ©o toÃ n bá»™ dá»¯ liá»‡u"""
        print(f"ğŸš€ Báº¯t Ä‘áº§u kÃ©o TOÃ€N Bá»˜ dá»¯ liá»‡u cho {len(self.provinces)} tá»‰nh")
        
        for i, province in enumerate(self.provinces, 1):
            print(f"\nğŸ›ï¸  [{i}/{len(self.provinces)}] ", end='')
            province_data = self.crawl_province_data(province)
            self.data.extend(province_data)
            
            # Checkpoint má»—i 2 tá»‰nh
            if i % 2 == 0:
                self.save_checkpoint(f"checkpoint_tinh_{i}")
            
            time.sleep(1)  # Delay giá»¯a cÃ¡c tá»‰nh
        
        print(f"\nğŸ‰ HOÃ€N THÃ€NH Táº¤T Cáº¢! Tá»•ng cá»™ng: {len(self.data)} báº£n ghi")
        return self.data
    
    def save_checkpoint(self, filename_prefix):
        """LÆ°u checkpoint"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename_prefix}_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Checkpoint: {filename}")
    
    def save_to_excel(self, filename=None):
        """LÆ°u dá»¯ liá»‡u ra file Excel"""
        if not self.data:
            print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u")
            return
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"sap_nhap_real_{timestamp}.xlsx"
        
        df = pd.DataFrame(self.data)
        
        # LÃ m sáº¡ch dá»¯ liá»‡u
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].astype(str).apply(lambda x: re.sub(r'[^\x20-\x7E\u00A0-\uFFFF]', '', str(x)) if pd.notna(x) else '')
                df[col] = df[col].apply(lambda x: x[:32767] if len(str(x)) > 32767 else x)
                df[col] = df[col].fillna('')
        
        try:
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # Sheet dá»¯ liá»‡u chÃ­nh
                df.to_excel(writer, sheet_name='Dá»¯ liá»‡u sÃ¡p nháº­p', index=False)
                
                # Sheet thá»‘ng kÃª
                stats = []
                
                if 'cap_hanh_chinh' in df.columns:
                    cap_stats = df['cap_hanh_chinh'].value_counts()
                    for cap, count in cap_stats.items():
                        stats.append({'Loáº¡i': 'Cáº¥p hÃ nh chÃ­nh', 'GiÃ¡ trá»‹': cap, 'Sá»‘ lÆ°á»£ng': count})
                
                if 'ten_tinh' in df.columns:
                    tinh_count = df['ten_tinh'].nunique()
                    stats.append({'Loáº¡i': 'Tá»•ng sá»‘ tá»‰nh/thÃ nh', 'GiÃ¡ trá»‹': '', 'Sá»‘ lÆ°á»£ng': tinh_count})
                
                if 'so_luong_thay_doi' in df.columns:
                    total_changes = df['so_luong_thay_doi'].sum()
                    stats.append({'Loáº¡i': 'Tá»•ng sá»‘ thay Ä‘á»•i', 'GiÃ¡ trá»‹': '', 'Sá»‘ lÆ°á»£ng': total_changes})
                
                # Thá»‘ng kÃª cÃ³ thÃ´ng tin sÃ¡p nháº­p
                df_with_info = df[(df['truoc_sap_nhap'] != '') | (df['sau_sap_nhap'] != '')]
                stats.append({'Loáº¡i': 'CÃ³ thÃ´ng tin sÃ¡p nháº­p', 'GiÃ¡ trá»‹': '', 'Sá»‘ lÆ°á»£ng': len(df_with_info)})
                
                if stats:
                    stats_df = pd.DataFrame(stats)
                    stats_df.to_excel(writer, sheet_name='Thá»‘ng kÃª', index=False)
                
                # Sheet chi tiáº¿t cÃ³ thÃ´ng tin sÃ¡p nháº­p
                if len(df_with_info) > 0:
                    df_with_info.to_excel(writer, sheet_name='CÃ³ thÃ´ng tin sÃ¡p nháº­p', index=False)
                
                # Sheet lá»—i
                if self.failed_requests:
                    error_df = pd.DataFrame(self.failed_requests)
                    error_df.to_excel(writer, sheet_name='Lá»—i request', index=False)
            
            print(f"ğŸ’¾ ÄÃ£ lÆ°u: {filename}")
            
        except Exception as e:
            print(f"âŒ Lá»—i lÆ°u Excel: {e}")
            # Fallback CSV
            csv_filename = filename.replace('.xlsx', '.csv')
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ ÄÃ£ lÆ°u CSV: {csv_filename}")
            filename = csv_filename
        
        return filename

def main():
    """HÃ m chÃ­nh"""
    print("=== TOOL KÃ‰O Dá»® LIá»†U Äá»ŠA CHá»ˆ SÃP NHáº¬P - PHIÃŠN Báº¢N THá»°C Táº¾ ===")
    print("ğŸŒ Website: https://thuvienphapluat.vn")
    print("ğŸ¯ Má»¥c tiÃªu: KÃ©o dá»¯ liá»‡u Ä‘á»‹a chá»‰ sÃ¡p nháº­p tá»« trang cÃ³ dá»¯ liá»‡u thá»±c")
    print("=" * 70)
    
    print("\nğŸ“‹ TÃ¹y chá»n:")
    print("1. ğŸ§ª Test vá»›i 2 tá»‰nh, 5 xÃ£/tá»‰nh")
    print("2. ğŸ“Š KÃ©o dá»¯ liá»‡u táº¥t cáº£ tá»‰nh cÃ³ sáºµn")
    
    try:
        choice = input("\nâ¤ Nháº­p lá»±a chá»n (1-2): ").strip()
        
        crawler = SapNhapCrawlerReal()
        
        if choice == "1":
            data = crawler.crawl_sample_data(max_provinces=2, max_xa_per_province=5)
            
        elif choice == "2":
            confirm = input("âš ï¸  Cáº£nh bÃ¡o: Sáº½ kÃ©o dá»¯ liá»‡u tá»« nhiá»u tá»‰nh. CÃ³ thá»ƒ máº¥t 1-2 giá». Tiáº¿p tá»¥c? (y/N): ").strip().lower()
            if confirm == 'y':
                data = crawler.crawl_full_data()
            else:
                print("âŒ ÄÃ£ há»§y.")
                return
        else:
            print("âš ï¸  Lá»±a chá»n khÃ´ng há»£p lá»‡. Cháº¡y test máº·c Ä‘á»‹nh.")
            data = crawler.crawl_sample_data()
        
        # LÆ°u káº¿t quáº£
        if data:
            filename = crawler.save_to_excel()
            
            print(f"\nğŸ“Š === Káº¾T QUáº¢ CUá»I CÃ™NG ===")
            df = pd.DataFrame(data)
            
            print(f"ğŸ“ˆ Tá»•ng sá»‘ báº£n ghi: {len(df)}")
            
            if 'cap_hanh_chinh' in df.columns:
                print(f"\nğŸ›ï¸  PhÃ¢n loáº¡i theo cáº¥p:")
                for cap, count in df['cap_hanh_chinh'].value_counts().items():
                    print(f"   {cap}: {count}")
            
            if 'ten_tinh' in df.columns:
                print(f"\nğŸ—ºï¸  Sá»‘ tá»‰nh/thÃ nh: {df['ten_tinh'].nunique()}")
                
            # Thá»‘ng kÃª cÃ³ thÃ´ng tin sÃ¡p nháº­p
            df_with_info = df[(df['truoc_sap_nhap'] != '') | (df['sau_sap_nhap'] != '')]
            print(f"ğŸ“‹ CÃ³ thÃ´ng tin sÃ¡p nháº­p: {len(df_with_info)}")
            
            if 'so_luong_thay_doi' in df.columns:
                total_changes = df['so_luong_thay_doi'].sum()
                print(f"ğŸ”„ Tá»•ng sá»‘ thay Ä‘á»•i: {total_changes}")
            
            print(f"\nğŸ’¾ File káº¿t quáº£: {filename}")
            
            if crawler.failed_requests:
                print(f"âš ï¸  Sá»‘ request lá»—i: {len(crawler.failed_requests)}")
            
            print(f"\nğŸ‰ HOÃ€N THÃ€NH THÃ€NH CÃ”NG!")
            
        else:
            print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u nÃ o Ä‘Æ°á»£c kÃ©o vá»")
            
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ÄÃ£ dá»«ng theo yÃªu cáº§u ngÆ°á»i dÃ¹ng")
        if 'crawler' in locals() and crawler.data:
            print("ğŸ’¾ Äang lÆ°u dá»¯ liá»‡u Ä‘Ã£ kÃ©o Ä‘Æ°á»£c...")
            crawler.save_to_excel()
            
    except Exception as e:
        print(f"\nâŒ Lá»—i: {e}")
        import traceback
        traceback.print_exc()
        
        if 'crawler' in locals() and crawler.data:
            print("ğŸ’¾ Äang lÆ°u dá»¯ liá»‡u Ä‘Ã£ kÃ©o Ä‘Æ°á»£c...")
            crawler.save_to_excel()

if __name__ == "__main__":
    main()
