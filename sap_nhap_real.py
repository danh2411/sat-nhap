#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script th·ª±c t·∫ø ƒë·ªÉ k√©o d·ªØ li·ªáu - s·ª≠ d·ª•ng c√°ch ti·∫øp c·∫≠n th·ª±c t·∫ø
"""

import requests
import json
import pandas as pd
import time
from bs4 import BeautifulSoup
from urllib.parse import parse_qs, urlparse
import re
import os
from datetime import datetime
import html

class SapNhapCrawlerReal:
    def __init__(self):
        self.base_url = "https://thuvienphapluat.vn"
        self.search_url = "https://thuvienphapluat.vn/ma-so-thue/tra-cuu-thong-tin-sap-nhap-tinh"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.data = []
        self.failed_requests = []
        
        # Danh s√°ch t·ªânh/th√†nh c·ªë ƒë·ªãnh
        self.provinces = [
            {'ma_tinh': '01', 'ten_tinh': 'H√† N·ªôi'},
            {'ma_tinh': '79', 'ten_tinh': 'H·ªì Ch√≠ Minh'}, 
            {'ma_tinh': '83', 'ten_tinh': 'Vƒ©nh Long'},  # ƒê√£ test c√≥ d·ªØ li·ªáu
            {'ma_tinh': '89', 'ten_tinh': 'An Giang'},
            {'ma_tinh': '92', 'ten_tinh': 'C·∫ßn Th∆°'},
        ]
        
    def get_page_content(self, url):
        """L·∫•y n·ªôi dung trang web"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except requests.RequestException as e:
            self.failed_requests.append({'url': url, 'error': str(e)})
            return None
    
    def discover_xa_phuong_from_known_page(self, ma_tinh):
        """Kh√°m ph√° danh s√°ch x√£/ph∆∞·ªùng t·ª´ trang ƒë√£ bi·∫øt c√≥ d·ªØ li·ªáu"""
        # Danh s√°ch c√°c m√£ x√£/ph∆∞·ªùng m·∫´u ƒë·ªÉ th·ª≠
        sample_xa_codes = ['28996', '28756', '28757', '1', '100', '200']
        
        xa_phuong_list = []
        found_data = False
        
        for sample_xa in sample_xa_codes:
            if found_data:
                break
                
            sample_url = f"{self.search_url}?MaTinh={ma_tinh}&MaXa={sample_xa}"
            content = self.get_page_content(sample_url)
            
            if not content:
                continue
            
            soup = BeautifulSoup(content, 'html.parser')
            
            # T√¨m t·∫•t c·∫£ link c√≥ MaXa
            xa_links = soup.find_all('a', href=re.compile(r'MaXa=\d+'))
            
            if xa_links:
                print(f"    üîç T√¨m th·∫•y {len(xa_links)} link t·ª´ m·∫´u MaXa={sample_xa}")
                found_data = True
                
                seen_xa = set()
                for link in xa_links:
                    href = link.get('href', '')
                    if f'MaTinh={ma_tinh}' in href:
                        parsed = urlparse(href)
                        query_params = parse_qs(parsed.query)
                        if 'MaXa' in query_params:
                            ma_xa = query_params['MaXa'][0]
                            if ma_xa not in seen_xa:
                                seen_xa.add(ma_xa)
                                text = link.text.strip()
                                text = re.sub(r'^\d+\.\s*', '', text)
                                xa_phuong_list.append({
                                    'ma_xa': ma_xa,
                                    'ten_xa': text
                                })
                break
        
        if not found_data:
            print(f"    ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu x√£/ph∆∞·ªùng t·ª´ c√°c m·∫´u")
        
    def discover_xa_phuong_by_scanning(self, ma_tinh, max_scan=50):
        """Qu√©t theo range ƒë·ªÉ t√¨m m√£ x√£/ph∆∞·ªùng h·ª£p l·ªá"""
        print(f"    üîç Qu√©t t√¨m m√£ x√£ h·ª£p l·ªá cho t·ªânh {ma_tinh}...")
        
        valid_xa = []
        scan_ranges = [
            range(1, 100),           # M√£ nh·ªè
            range(1000, 1100),       # M√£ trung b√¨nh
            range(28000, 29000),     # M√£ theo pattern ƒë√£ th·∫•y
            range(10000, 10100),     # M√£ kh√°c
        ]
        
        found_count = 0
        
        for scan_range in scan_ranges:
            if found_count >= max_scan:
                break
                
            for ma_xa in scan_range:
                if found_count >= max_scan:
                    break
                    
                test_url = f"{self.search_url}?MaTinh={ma_tinh}&MaXa={ma_xa}"
                content = self.get_page_content(test_url)
                
                if content:
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # Ki·ªÉm tra xem c√≥ th√¥ng tin th·ª±c s·ª± kh√¥ng
                    text = soup.get_text().lower()
                    if any(keyword in text for keyword in ['s√°p nh·∫≠p', 'ph∆∞·ªùng', 'x√£', 'th·ªã tr·∫•n']):
                        # C√≥ v·∫ª nh∆∞ c√≥ d·ªØ li·ªáu th·ª±c
                        title_tag = soup.find('title')
                        if title_tag:
                            title = title_tag.text.strip()
                            if 't·ªânh' not in title.lower():  # Kh√¥ng ph·∫£i trang t·ªïng qu√°t
                                valid_xa.append({
                                    'ma_xa': str(ma_xa),
                                    'ten_xa': f'ƒê∆°n v·ªã h√†nh ch√≠nh {ma_xa}',
                                    'url': test_url
                                })
                                found_count += 1
                                print(f"      ‚úì T√¨m th·∫•y MaXa={ma_xa}")
                
                time.sleep(0.1)  # Delay nh·ªè
        
        print(f"    üìä T√¨m ƒë∆∞·ª£c {found_count} m√£ x√£ h·ª£p l·ªá")
        return valid_xa
    
    def parse_sap_nhap_info(self, soup):
        """Ph√¢n t√≠ch th√¥ng tin s√°p nh·∫≠p"""
        info = {
            'truoc_sap_nhap': '',
            'sau_sap_nhap': '',
            'chi_tiet': []
        }
        
        # T√¨m b·∫£ng c√≥ th√¥ng tin s√°p nh·∫≠p
        tables = soup.find_all('table')
        
        for table in tables:
            rows = table.find_all('tr')
            
            for i, row in enumerate(rows):
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    cell_texts = [cell.get_text(strip=True).lower() for cell in cells]
                    
                    has_truoc = any('tr∆∞·ªõc s√°p nh·∫≠p' in text for text in cell_texts)
                    has_sau = any('sau s√°p nh·∫≠p' in text for text in cell_texts)
                    
                    if has_truoc and has_sau:
                        truoc_col = next((j for j, text in enumerate(cell_texts) if 'tr∆∞·ªõc s√°p nh·∫≠p' in text), 0)
                        sau_col = next((j for j, text in enumerate(cell_texts) if 'sau s√°p nh·∫≠p' in text), 1)
                        
                        for j in range(i + 1, len(rows)):
                            data_row = rows[j]
                            data_cells = data_row.find_all(['td', 'th'])
                            
                            if len(data_cells) > max(truoc_col, sau_col):
                                truoc_text = html.unescape(data_cells[truoc_col].get_text(strip=True))
                                sau_text = html.unescape(data_cells[sau_col].get_text(strip=True))
                                
                                if truoc_text and sau_text:
                                    info['chi_tiet'].append({
                                        'truoc': truoc_text,
                                        'sau': sau_text
                                    })
                                    
                                    if not info['truoc_sap_nhap']:
                                        info['truoc_sap_nhap'] = truoc_text
                                    if not info['sau_sap_nhap']:
                                        info['sau_sap_nhap'] = sau_text
                        break
        
        return info
    
    def get_sap_nhap_details(self, ma_tinh, ma_xa=None, ten_tinh='', ten_xa=''):
        """L·∫•y chi ti·∫øt th√¥ng tin s√°p nh·∫≠p"""
        if ma_xa:
            url = f"{self.search_url}?MaTinh={ma_tinh}&MaXa={ma_xa}"
            cap_hanh_chinh = 'X√£/Ph∆∞·ªùng'
        else:
            url = f"{self.search_url}?MaTinh={ma_tinh}"
            cap_hanh_chinh = 'T·ªânh/Th√†nh ph·ªë'
        
        content = self.get_page_content(url)
        if not content:
            return None
        
        soup = BeautifulSoup(content, 'html.parser')
        sap_nhap_info = self.parse_sap_nhap_info(soup)
        
        result = {
            'ma_tinh': ma_tinh,
            'ten_tinh': ten_tinh,
            'ma_xa': ma_xa or '',
            'ten_xa': ten_xa,
            'cap_hanh_chinh': cap_hanh_chinh,
            'url': url,
            'truoc_sap_nhap': sap_nhap_info['truoc_sap_nhap'],
            'sau_sap_nhap': sap_nhap_info['sau_sap_nhap'],
            'chi_tiet_json': json.dumps(sap_nhap_info['chi_tiet'], ensure_ascii=False) if sap_nhap_info['chi_tiet'] else '',
            'so_luong_thay_doi': len(sap_nhap_info['chi_tiet'])
        }
        
        return result
    
    def crawl_province_data(self, province, max_xa=None):
        """K√©o d·ªØ li·ªáu cho m·ªôt t·ªânh"""
        ma_tinh = province['ma_tinh']
        ten_tinh = province['ten_tinh']
        
        print(f"\n--- X·ª≠ l√Ω {ten_tinh} (M√£: {ma_tinh}) ---")
        
        province_data = []
        
        # L·∫•y th√¥ng tin c·∫•p t·ªânh
        tinh_info = self.get_sap_nhap_details(ma_tinh, ten_tinh=ten_tinh)
        if tinh_info:
            province_data.append(tinh_info)
            print(f"‚úì L·∫•y ƒë∆∞·ª£c th√¥ng tin t·ªânh")
        
        # Kh√°m ph√° danh s√°ch x√£/ph∆∞·ªùng
        xa_phuong_list = self.discover_xa_phuong_from_known_page(ma_tinh)
        
        # N·∫øu kh√¥ng t√¨m th·∫•y b·∫±ng c√°ch 1, th·ª≠ qu√©t
        if not xa_phuong_list:
            print(f"    üîç Kh√¥ng t√¨m th·∫•y t·ª´ m·∫´u, th·ª≠ qu√©t...")
            xa_phuong_list = self.discover_xa_phuong_by_scanning(ma_tinh, max_scan=20)
        
        print(f"üîç T√¨m th·∫•y {len(xa_phuong_list)} x√£/ph∆∞·ªùng")
        
        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ƒë·ªÉ test
        if max_xa and len(xa_phuong_list) > max_xa:
            xa_phuong_list = xa_phuong_list[:max_xa]
            print(f"üìä Gi·ªõi h·∫°n {max_xa} x√£ ƒë·∫ßu ti√™n ƒë·ªÉ test")
        
        # X·ª≠ l√Ω t·ª´ng x√£/ph∆∞·ªùng
        success_count = 0
        for i, xa_phuong in enumerate(xa_phuong_list):
            ma_xa = xa_phuong['ma_xa']
            ten_xa = xa_phuong['ten_xa']
            
            print(f"  üìç [{i+1}/{len(xa_phuong_list)}] {ten_xa} (M√£: {ma_xa})")
            
            xa_info = self.get_sap_nhap_details(ma_tinh, ma_xa, ten_tinh, ten_xa)
            if xa_info:
                province_data.append(xa_info)
                success_count += 1
                
                # Hi·ªÉn th·ªã th√¥ng tin n·∫øu c√≥
                if xa_info['truoc_sap_nhap'] or xa_info['sau_sap_nhap']:
                    print(f"    ‚úì C√≥ th√¥ng tin s√°p nh·∫≠p")
            
            time.sleep(0.3)  # Delay
            
            # Status update m·ªói 10 item
            if (i + 1) % 10 == 0:
                print(f"    üìà ƒê√£ x·ª≠ l√Ω {i+1}/{len(xa_phuong_list)}")
        
        print(f"‚úÖ Ho√†n th√†nh {ten_tinh}: {len(province_data)} b·∫£n ghi ({success_count} x√£/ph∆∞·ªùng)")
        return province_data
    
    def crawl_sample_data(self, max_provinces=2, max_xa_per_province=10):
        """K√©o d·ªØ li·ªáu m·∫´u"""
        print(f"üöÄ B·∫Øt ƒë·∫ßu k√©o d·ªØ li·ªáu m·∫´u: {max_provinces} t·ªânh, t·ªëi ƒëa {max_xa_per_province} x√£/t·ªânh")
        
        sample_provinces = self.provinces[:max_provinces]
        
        for province in sample_provinces:
            province_data = self.crawl_province_data(province, max_xa_per_province)
            self.data.extend(province_data)
        
        print(f"üéâ Ho√†n th√†nh! T·ªïng c·ªông: {len(self.data)} b·∫£n ghi")
        return self.data
    
    def crawl_full_data(self):
        """K√©o to√†n b·ªô d·ªØ li·ªáu"""
        print(f"üöÄ B·∫Øt ƒë·∫ßu k√©o TO√ÄN B·ªò d·ªØ li·ªáu cho {len(self.provinces)} t·ªânh")
        
        for i, province in enumerate(self.provinces, 1):
            print(f"\nüèõÔ∏è  [{i}/{len(self.provinces)}] ", end='')
            province_data = self.crawl_province_data(province)
            self.data.extend(province_data)
            
            # Checkpoint m·ªói 2 t·ªânh
            if i % 2 == 0:
                self.save_checkpoint(f"checkpoint_tinh_{i}")
            
            time.sleep(1)  # Delay gi·ªØa c√°c t·ªânh
        
        print(f"\nüéâ HO√ÄN TH√ÄNH T·∫§T C·∫¢! T·ªïng c·ªông: {len(self.data)} b·∫£n ghi")
        return self.data
    
    def save_checkpoint(self, filename_prefix):
        """L∆∞u checkpoint"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename_prefix}_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ Checkpoint: {filename}")
    
    def save_to_excel(self, filename=None):
        """L∆∞u d·ªØ li·ªáu ra file Excel"""
        if not self.data:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u")
            return
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"sap_nhap_real_{timestamp}.xlsx"
        
        df = pd.DataFrame(self.data)
        
        # L√†m s·∫°ch d·ªØ li·ªáu
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].astype(str).apply(lambda x: re.sub(r'[^\x20-\x7E\u00A0-\uFFFF]', '', str(x)) if pd.notna(x) else '')
                df[col] = df[col].apply(lambda x: x[:32767] if len(str(x)) > 32767 else x)
                df[col] = df[col].fillna('')
        
        try:
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # Sheet d·ªØ li·ªáu ch√≠nh
                df.to_excel(writer, sheet_name='D·ªØ li·ªáu s√°p nh·∫≠p', index=False)
                
                # Sheet th·ªëng k√™
                stats = []
                
                if 'cap_hanh_chinh' in df.columns:
                    cap_stats = df['cap_hanh_chinh'].value_counts()
                    for cap, count in cap_stats.items():
                        stats.append({'Lo·∫°i': 'C·∫•p h√†nh ch√≠nh', 'Gi√° tr·ªã': cap, 'S·ªë l∆∞·ª£ng': count})
                
                if 'ten_tinh' in df.columns:
                    tinh_count = df['ten_tinh'].nunique()
                    stats.append({'Lo·∫°i': 'T·ªïng s·ªë t·ªânh/th√†nh', 'Gi√° tr·ªã': '', 'S·ªë l∆∞·ª£ng': tinh_count})
                
                if 'so_luong_thay_doi' in df.columns:
                    total_changes = df['so_luong_thay_doi'].sum()
                    stats.append({'Lo·∫°i': 'T·ªïng s·ªë thay ƒë·ªïi', 'Gi√° tr·ªã': '', 'S·ªë l∆∞·ª£ng': total_changes})
                
                # Th·ªëng k√™ c√≥ th√¥ng tin s√°p nh·∫≠p
                df_with_info = df[(df['truoc_sap_nhap'] != '') | (df['sau_sap_nhap'] != '')]
                stats.append({'Lo·∫°i': 'C√≥ th√¥ng tin s√°p nh·∫≠p', 'Gi√° tr·ªã': '', 'S·ªë l∆∞·ª£ng': len(df_with_info)})
                
                if stats:
                    stats_df = pd.DataFrame(stats)
                    stats_df.to_excel(writer, sheet_name='Th·ªëng k√™', index=False)
                
                # Sheet chi ti·∫øt c√≥ th√¥ng tin s√°p nh·∫≠p
                if len(df_with_info) > 0:
                    df_with_info.to_excel(writer, sheet_name='C√≥ th√¥ng tin s√°p nh·∫≠p', index=False)
                
                # Sheet l·ªói
                if self.failed_requests:
                    error_df = pd.DataFrame(self.failed_requests)
                    error_df.to_excel(writer, sheet_name='L·ªói request', index=False)
            
            print(f"üíæ ƒê√£ l∆∞u: {filename}")
            
        except Exception as e:
            print(f"‚ùå L·ªói l∆∞u Excel: {e}")
            # Fallback CSV
            csv_filename = filename.replace('.xlsx', '.csv')
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"üíæ ƒê√£ l∆∞u CSV: {csv_filename}")
            filename = csv_filename
        
        return filename

def main():
    """H√†m ch√≠nh"""
    print("=== TOOL K√âO D·ªÆ LI·ªÜU ƒê·ªäA CH·ªà S√ÅP NH·∫¨P - PHI√äN B·∫¢N TH·ª∞C T·∫æ ===")
    print("üåê Website: https://thuvienphapluat.vn")
    print("üéØ M·ª•c ti√™u: K√©o d·ªØ li·ªáu ƒë·ªãa ch·ªâ s√°p nh·∫≠p t·ª´ trang c√≥ d·ªØ li·ªáu th·ª±c")
    print("=" * 70)
    
    print("\nüìã T√πy ch·ªçn:")
    print("1. üß™ Test v·ªõi 2 t·ªânh, 5 x√£/t·ªânh")
    print("2. üìä K√©o d·ªØ li·ªáu t·∫•t c·∫£ t·ªânh c√≥ s·∫µn")
    
    try:
        choice = input("\n‚û§ Nh·∫≠p l·ª±a ch·ªçn (1-2): ").strip()
        
        crawler = SapNhapCrawlerReal()
        
        if choice == "1":
            data = crawler.crawl_sample_data(max_provinces=2, max_xa_per_province=5)
            
        elif choice == "2":
            confirm = input("‚ö†Ô∏è  C·∫£nh b√°o: S·∫Ω k√©o d·ªØ li·ªáu t·ª´ nhi·ªÅu t·ªânh. C√≥ th·ªÉ m·∫•t 1-2 gi·ªù. Ti·∫øp t·ª•c? (y/N): ").strip().lower()
            if confirm == 'y':
                data = crawler.crawl_full_data()
            else:
                print("‚ùå ƒê√£ h·ªßy.")
                return
        else:
            print("‚ö†Ô∏è  L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá. Ch·∫°y test m·∫∑c ƒë·ªãnh.")
            data = crawler.crawl_sample_data()
        
        # L∆∞u k·∫øt qu·∫£
        if data:
            filename = crawler.save_to_excel()
            
            print(f"\nüìä === K·∫æT QU·∫¢ CU·ªêI C√ôNG ===")
            df = pd.DataFrame(data)
            
            print(f"üìà T·ªïng s·ªë b·∫£n ghi: {len(df)}")
            
            if 'cap_hanh_chinh' in df.columns:
                print(f"\nüèõÔ∏è  Ph√¢n lo·∫°i theo c·∫•p:")
                for cap, count in df['cap_hanh_chinh'].value_counts().items():
                    print(f"   {cap}: {count}")
            
            if 'ten_tinh' in df.columns:
                print(f"\nüó∫Ô∏è  S·ªë t·ªânh/th√†nh: {df['ten_tinh'].nunique()}")
                
            # Th·ªëng k√™ c√≥ th√¥ng tin s√°p nh·∫≠p
            df_with_info = df[(df['truoc_sap_nhap'] != '') | (df['sau_sap_nhap'] != '')]
            print(f"üìã C√≥ th√¥ng tin s√°p nh·∫≠p: {len(df_with_info)}")
            
            if 'so_luong_thay_doi' in df.columns:
                total_changes = df['so_luong_thay_doi'].sum()
                print(f"üîÑ T·ªïng s·ªë thay ƒë·ªïi: {total_changes}")
            
            print(f"\nüíæ File k·∫øt qu·∫£: {filename}")
            
            if crawler.failed_requests:
                print(f"‚ö†Ô∏è  S·ªë request l·ªói: {len(crawler.failed_requests)}")
            
            print(f"\nüéâ HO√ÄN TH√ÄNH TH√ÄNH C√îNG!")
            
        else:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c k√©o v·ªÅ")
            
    except KeyboardInterrupt:
        print("\n\nüõë ƒê√£ d·ª´ng theo y√™u c·∫ßu ng∆∞·ªùi d√πng")
        if 'crawler' in locals() and crawler.data:
            print("üíæ ƒêang l∆∞u d·ªØ li·ªáu ƒë√£ k√©o ƒë∆∞·ª£c...")
            crawler.save_to_excel()
            
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()
        
        if 'crawler' in locals() and crawler.data:
            print("üíæ ƒêang l∆∞u d·ªØ li·ªáu ƒë√£ k√©o ƒë∆∞·ª£c...")
            crawler.save_to_excel()

if __name__ == "__main__":
    main()
