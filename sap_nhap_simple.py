
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script cuá»‘i cÃ¹ng - sá»­ dá»¥ng dá»¯ liá»‡u cÃ³ sáºµn tá»« website
"""

import requests
import json
import pandas as pd
import time
import os
from bs4 import BeautifulSoup
import re
import html
from datetime import datetime

class SapNhapCrawlerSimple:
    def __init__(self):
        self.base_url = "https://thuvienphapluat.vn"
        self.search_url = "https://thuvienphapluat.vn/ma-so-thue/tra-cuu-thong-tin-sap-nhap-tinh"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
        })
        self.data = []
        self.provinces = []
        self.xa_phuong_cache = {}
        self.error_log = []  # LÆ°u cÃ¡c lá»—i Ä‘á»ƒ xá»­ lÃ½ láº¡i
        self.stats = {
            'total_processed': 0,
            'success_count': 0,
            'error_count': 0,
            'rate_limit_count': 0,
            'timeout_count': 0,
            'connection_error_count': 0
        }
    
    def get_provinces_from_html(self):
        """Láº¥y danh sÃ¡ch tá»‰nh tá»« dropdown HTML"""
        print("ğŸŒ Äang láº¥y danh sÃ¡ch tá»‰nh tá»« trang web...")
        
        content = self.get_page_content(self.search_url)
        if not content:
            print("âŒ KhÃ´ng thá»ƒ láº¥y trang chÃ­nh")
            return []
        
        soup = BeautifulSoup(content, 'html.parser')
        provinces = []
        
        # TÃ¬m select box cho tá»‰nh - cÃ³ thá»ƒ cÃ³ id="tinh-cu" hoáº·c tÆ°Æ¡ng tá»±
        selects = soup.find_all('select')
        
        for select in selects:
            options = select.find_all('option')
            
            # Kiá»ƒm tra xem cÃ³ pháº£i select tá»‰nh khÃ´ng báº±ng cÃ¡ch xem ná»™i dung option
            is_province_select = False
            for option in options:
                if option.get('value') and option.text:
                    # Kiá»ƒm tra pattern tÃªn tá»‰nh
                    text = option.text.strip()
                    if any(keyword in text.lower() for keyword in ['hÃ  ná»™i', 'há»“ chÃ­ minh', 'báº¿n tre', 'vÄ©nh long']):
                        is_province_select = True
                        break
            
            if is_province_select:
                print(f"âœ… TÃ¬m tháº¥y select box tá»‰nh vá»›i {len(options)} option")
                
                for option in options:
                    value = option.get('value', '').strip()
                    text = option.text.strip()
                    
                    # Bá» qua option máº·c Ä‘á»‹nh
                    if value and value != '0' and text and not text.startswith('--'):
                        # TrÃ­ch xuáº¥t mÃ£ tá»‰nh tá»« value hoáº·c id
                        ma_tinh = value
                        
                        # LÃ m sáº¡ch tÃªn tá»‰nh
                        ten_tinh = text.strip()
                        
                        provinces.append({
                            'ma_tinh': ma_tinh,
                            'ten_tinh': ten_tinh
                        })
                
                break  # ÄÃ£ tÃ¬m tháº¥y select tá»‰nh
        
        print(f"ğŸ“Š TÃ¬m tháº¥y {len(provinces)} tá»‰nh/thÃ nh phá»‘")
        
        # Hiá»ƒn thá»‹ má»™t vÃ i tá»‰nh Ä‘áº§u tiÃªn
        for i, province in enumerate(provinces[:5]):
            print(f"  {i+1}. {province['ma_tinh']}: {province['ten_tinh']}")
        
        if len(provinces) > 5:
            print(f"  ... vÃ  {len(provinces) - 5} tá»‰nh khÃ¡c")
        
        return provinces
    
    def get_xa_phuong_from_province(self, ma_tinh, ten_tinh=''):
        """Láº¥y danh sÃ¡ch xÃ£/phÆ°á»ng tá»« má»™t tá»‰nh cá»¥ thá»ƒ"""
        
        # Kiá»ƒm tra cache
        if ma_tinh in self.xa_phuong_cache:
            return self.xa_phuong_cache[ma_tinh]
        
        print(f"  ğŸ” Äang láº¥y danh sÃ¡ch xÃ£/phÆ°á»ng cho tá»‰nh {ma_tinh}...")
        
        # Truy cáº­p trang vá»›i mÃ£ tá»‰nh
        url = f"{self.search_url}?MaTinh={ma_tinh}"
        content = self.get_page_content(url, ma_tinh=ma_tinh, ten_tinh=ten_tinh)
        
        if not content:
            return []
        
        soup = BeautifulSoup(content, 'html.parser')
        xa_phuong_list = []
        
        # TÃ¬m select box cho xÃ£/phÆ°á»ng
        selects = soup.find_all('select')
        
        for select in selects:
            options = select.find_all('option')
            
            # Kiá»ƒm tra xem cÃ³ pháº£i select xÃ£/phÆ°á»ng khÃ´ng
            is_xa_select = False
            for option in options:
                if option.text and any(keyword in option.text.lower() for keyword in ['phÆ°á»ng', 'xÃ£', 'thá»‹ tráº¥n']):
                    is_xa_select = True
                    break
            
            if is_xa_select:
                for option in options:
                    value = option.get('value', '').strip()
                    text = option.text.strip()
                    
                    # Bá» qua option máº·c Ä‘á»‹nh
                    if value and value != '0' and text and not text.startswith('--'):
                        xa_phuong_list.append({
                            'ma_xa': value,
                            'ten_xa': text
                        })
                
                break
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y trong select, thá»­ tÃ¬m trong cÃ¡c link
        if not xa_phuong_list:
            links = soup.find_all('a', href=re.compile(r'MaXa=\d+'))
            seen_xa = set()
            
            for link in links:
                href = link.get('href', '')
                if f'MaTinh={ma_tinh}' in href:
                    # TrÃ­ch xuáº¥t mÃ£ xÃ£ tá»« URL
                    match = re.search(r'MaXa=(\d+)', href)
                    if match:
                        ma_xa = match.group(1)
                        if ma_xa not in seen_xa:
                            seen_xa.add(ma_xa)
                            text = link.text.strip()
                            
                            # LÃ m sáº¡ch text
                            text = re.sub(r'^\d+\.\s*', '', text)
                            
                            xa_phuong_list.append({
                                'ma_xa': ma_xa,
                                'ten_xa': text
                            })
        
        print(f"    ğŸ“ TÃ¬m tháº¥y {len(xa_phuong_list)} xÃ£/phÆ°á»ng")
        
        # Cache káº¿t quáº£
        self.xa_phuong_cache[ma_tinh] = xa_phuong_list
        
        return xa_phuong_list
        """Láº¥y ná»™i dung trang web"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            print(f"âŒ Lá»—i truy cáº­p {url}: {e}")
            return None
    
    def parse_sap_nhap_info(self, soup):
        """PhÃ¢n tÃ­ch thÃ´ng tin sÃ¡p nháº­p tá»« trang"""
        info = {
            'truoc_sap_nhap': '',
            'sau_sap_nhap': '',
            'chi_tiet': []
        }
        
        # TÃ¬m báº£ng cÃ³ thÃ´ng tin sÃ¡p nháº­p
        tables = soup.find_all('table')
        
        for table in tables:
            rows = table.find_all('tr')
            
            for i, row in enumerate(rows):
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    cell_texts = [cell.get_text(strip=True).lower() for cell in cells]
                    
                    # Kiá»ƒm tra náº¿u cÃ³ header trÆ°á»›c/sau sÃ¡p nháº­p
                    has_truoc = any('trÆ°á»›c sÃ¡p nháº­p' in text for text in cell_texts)
                    has_sau = any('sau sÃ¡p nháº­p' in text for text in cell_texts)
                    
                    if has_truoc and has_sau:
                        # TÃ¬m vá»‹ trÃ­ cá»™t
                        truoc_col = next((j for j, text in enumerate(cell_texts) if 'trÆ°á»›c sÃ¡p nháº­p' in text), 0)
                        sau_col = next((j for j, text in enumerate(cell_texts) if 'sau sÃ¡p nháº­p' in text), 1)
                        
                        # Láº¥y dá»¯ liá»‡u tá»« cÃ¡c row tiáº¿p theo
                        for j in range(i + 1, len(rows)):
                            data_row = rows[j]
                            data_cells = data_row.find_all(['td', 'th'])
                            
                            if len(data_cells) > max(truoc_col, sau_col):
                                truoc_text = html.unescape(data_cells[truoc_col].get_text(strip=True))
                                sau_text = html.unescape(data_cells[sau_col].get_text(strip=True))
                                
                                if truoc_text and sau_text and len(truoc_text) > 5 and len(sau_text) > 5:
                                    info['chi_tiet'].append({
                                        'truoc': truoc_text,
                                        'sau': sau_text
                                    })
                                    
                                    # Láº¥y thÃ´ng tin tá»•ng quan (dÃ²ng Ä‘áº§u tiÃªn)
                                    if not info['truoc_sap_nhap']:
                                        info['truoc_sap_nhap'] = truoc_text
                                    if not info['sau_sap_nhap']:
                                        info['sau_sap_nhap'] = sau_text
                        break
        
        return info
    
    def log_error(self, error_type, url, message, ma_tinh=None, ma_xa=None, ten_tinh=None, ten_xa=None):
        """Ghi log lá»—i Ä‘á»ƒ cÃ³ thá»ƒ xá»­ lÃ½ láº¡i sau"""
        error_entry = {
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'error_type': error_type,
            'url': url,
            'message': str(message),
            'ma_tinh': ma_tinh,
            'ma_xa': ma_xa,
            'ten_tinh': ten_tinh,
            'ten_xa': ten_xa
        }
        self.error_log.append(error_entry)
        
        # Cáº­p nháº­t stats
        self.stats['error_count'] += 1
        if error_type == 'rate_limit':
            self.stats['rate_limit_count'] += 1
        elif error_type == 'timeout':
            self.stats['timeout_count'] += 1
        elif error_type == 'connection_error':
            self.stats['connection_error_count'] += 1

    def get_page_content(self, url, ma_tinh=None, ma_xa=None, ten_tinh=None, ten_xa=None):
        """Láº¥y ná»™i dung trang web vá»›i retry logic"""
        max_retries = 3
        retry_delay = 2  # Start with 2 seconds
        
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=15)
                
                if response.status_code == 429:  # Too Many Requests
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)  # Exponential backoff
                        print(f"    â³ Rate limited. Chá» {wait_time}s rá»“i thá»­ láº¡i...")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"    âŒ Bá»‹ rate limit sau {max_retries} láº§n thá»­")
                        self.log_error('rate_limit', url, 'Too Many Requests after retries', ma_tinh, ma_xa, ten_tinh, ten_xa)
                        return None
                
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
                
            except requests.exceptions.Timeout as e:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)
                    print(f"    â³ Timeout, chá» {wait_time}s rá»“i thá»­ láº¡i...")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"    âŒ Timeout sau {max_retries} láº§n thá»­")
                    self.log_error('timeout', url, str(e), ma_tinh, ma_xa, ten_tinh, ten_xa)
                    return None
                    
            except requests.exceptions.ConnectionError as e:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)
                    print(f"    â³ Lá»—i káº¿t ná»‘i, chá» {wait_time}s rá»“i thá»­ láº¡i...")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"    âŒ Lá»—i káº¿t ná»‘i sau {max_retries} láº§n thá»­")
                    self.log_error('connection_error', url, str(e), ma_tinh, ma_xa, ten_tinh, ten_xa)
                    return None
                    
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)
                    print(f"    â³ Lá»—i request, chá» {wait_time}s rá»“i thá»­ láº¡i...")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"    âŒ Lá»—i request sau {max_retries} láº§n thá»­: {e}")
                    self.log_error('request_error', url, str(e), ma_tinh, ma_xa, ten_tinh, ten_xa)
                    return None
        
        return None
    
    def get_sap_nhap_details(self, ma_tinh, ma_xa=None, ten_tinh='', ten_xa=''):
        """Láº¥y chi tiáº¿t thÃ´ng tin sÃ¡p nháº­p"""
        if ma_xa:
            url = f"{self.search_url}?MaTinh={ma_tinh}&MaXa={ma_xa}"
            cap_hanh_chinh = 'XÃ£/PhÆ°á»ng'
        else:
            url = f"{self.search_url}?MaTinh={ma_tinh}"
            cap_hanh_chinh = 'Tá»‰nh/ThÃ nh phá»‘'
        
        print(f"  ğŸ“„ Äang láº¥y: {ten_xa or ten_tinh}")
        
        self.stats['total_processed'] += 1
        
        content = self.get_page_content(url, ma_tinh, ma_xa, ten_tinh, ten_xa)
        if not content:
            return None
        
        soup = BeautifulSoup(content, 'html.parser')
        sap_nhap_info = self.parse_sap_nhap_info(soup)
        
        # Kiá»ƒm tra xem cÃ³ thÃ´ng tin khÃ´ng
        has_info = bool(sap_nhap_info['truoc_sap_nhap'] or sap_nhap_info['sau_sap_nhap'] or sap_nhap_info['chi_tiet'])
        
        if has_info:
            print(f"    âœ… CÃ³ thÃ´ng tin sÃ¡p nháº­p!")
            self.stats['success_count'] += 1
        else:
            print(f"    âšª KhÃ´ng cÃ³ thÃ´ng tin sÃ¡p nháº­p")
        
        result = {
            'ma_tinh': ma_tinh,
            'ten_tinh': ten_tinh,
            'ma_xa': ma_xa or '',
            'ten_xa': ten_xa,
            'cap_hanh_chinh': cap_hanh_chinh,
            'url': url,
            'truoc_sap_nhap': sap_nhap_info['truoc_sap_nhap'],
            'sau_sap_nhap': sap_nhap_info['sau_sap_nhap'],
            'chi_tiet_json': json.dumps(sap_nhap_info['chi_tiet'], ensure_ascii=False) if sap_nhap_info['chi_tiet'] else '',
            'so_luong_thay_doi': len(sap_nhap_info['chi_tiet']),
            'co_thong_tin': has_info
        }
        
        return result
    
    def crawl_all_autodiscovery(self, max_provinces=None):
        """Tá»± Ä‘á»™ng tÃ¬m táº¥t cáº£ tá»‰nh vÃ  xÃ£/phÆ°á»ng tá»« website"""
        print("\nğŸ¤– === Báº®T Äáº¦U AUTO-DISCOVERY ===")
        
        # BÆ°á»›c 1: Láº¥y danh sÃ¡ch tá»‰nh
        provinces = self.get_provinces_from_html()
        
        if not provinces:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y tá»‰nh nÃ o. Fallback vá» dá»¯ liá»‡u máº«u.")
            return self.crawl_known_data()
        
        # Giá»›i háº¡n sá»‘ tá»‰nh náº¿u cáº§n
        if max_provinces and max_provinces < len(provinces):
            provinces = provinces[:max_provinces]
            print(f"âš ï¸  Giá»›i háº¡n xá»­ lÃ½ {max_provinces} tá»‰nh Ä‘áº§u tiÃªn")
        
        print(f"\nğŸ›ï¸  Sáº½ xá»­ lÃ½ {len(provinces)} tá»‰nh")
        
        total_processed = 0
        
        for i, province in enumerate(provinces, 1):
            ma_tinh = province['ma_tinh']
            ten_tinh = province['ten_tinh']
            
            print(f"\nğŸ“ [{i}/{len(provinces)}] Tá»‰nh: {ten_tinh} (MÃ£: {ma_tinh})")
            
            # Láº¥y danh sÃ¡ch xÃ£/phÆ°á»ng
            xa_phuong_list = self.get_xa_phuong_from_province(ma_tinh, ten_tinh)
            
            if not xa_phuong_list:
                print(f"  âš ï¸  KhÃ´ng cÃ³ xÃ£/phÆ°á»ng nÃ o cho tá»‰nh {ten_tinh}")
                continue
            
            print(f"  ğŸ“Š TÃ¬m tháº¥y {len(xa_phuong_list)} xÃ£/phÆ°á»ng")
            
            # Xá»­ lÃ½ tá»«ng xÃ£/phÆ°á»ng
            for j, xa in enumerate(xa_phuong_list, 1):
                ma_xa = xa['ma_xa']
                ten_xa = xa['ten_xa']
                
                if j <= 3 or j % 10 == 0:  # Hiá»ƒn thá»‹ progress
                    print(f"    [{j}/{len(xa_phuong_list)}] {ten_xa}")
                
                # Láº¥y thÃ´ng tin sÃ¡p nháº­p
                xa_info = self.get_sap_nhap_details(ma_tinh, ma_xa, ten_tinh, ten_xa)
                
                if xa_info:
                    self.data.append(xa_info)
                    total_processed += 1
                
                # Delay Ä‘á»ƒ trÃ¡nh bá»‹ block - tÄƒng delay cho auto-discovery
                time.sleep(1.2)  # TÄƒng tá»« 0.6s lÃªn 1.2s
            
            print(f"  âœ… HoÃ n thÃ nh tá»‰nh {ten_tinh}: {len(xa_phuong_list)} xÃ£/phÆ°á»ng")
            
            # Delay giá»¯a cÃ¡c tá»‰nh - tÄƒng delay
            if i < len(provinces):
                time.sleep(3)  # TÄƒng tá»« 1.5s lÃªn 3s
        
        print(f"\nğŸ‰ AUTO-DISCOVERY HOÃ€N THÃ€NH!")
        print(f"ğŸ“Š Tá»•ng cá»™ng xá»­ lÃ½: {total_processed} báº£n ghi tá»« {len(provinces)} tá»‰nh")
        
        return self.data

    def save_error_log(self, filename=None):
        """LÆ°u danh sÃ¡ch lá»—i ra file Ä‘á»ƒ xá»­ lÃ½ láº¡i"""
        if not self.error_log:
            print("ğŸ“‹ KhÃ´ng cÃ³ lá»—i nÃ o Ä‘á»ƒ lÆ°u")
            return None
            
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"error_log_{timestamp}.xlsx"
        
        try:
            error_df = pd.DataFrame(self.error_log)
            
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # Sheet lá»—i chi tiáº¿t
                error_df.to_excel(writer, sheet_name='Danh sÃ¡ch lá»—i', index=False)
                
                # Sheet thá»‘ng kÃª lá»—i
                error_stats = []
                error_type_counts = error_df['error_type'].value_counts()
                for error_type, count in error_type_counts.items():
                    error_stats.append({'Loáº¡i lá»—i': error_type, 'Sá»‘ lÆ°á»£ng': count})
                
                if error_stats:
                    stats_df = pd.DataFrame(error_stats)
                    stats_df.to_excel(writer, sheet_name='Thá»‘ng kÃª lá»—i', index=False)
                
                # Sheet URL retry
                retry_urls = error_df[['url', 'ma_tinh', 'ma_xa', 'ten_tinh', 'ten_xa']].drop_duplicates()
                retry_urls.to_excel(writer, sheet_name='URLs cáº§n retry', index=False)
            
            print(f"ğŸ“‹ ÄÃ£ lÆ°u error log: {filename}")
            return filename
            
        except Exception as e:
            print(f"âŒ Lá»—i lÆ°u error log: {e}")
            # Fallback CSV
            csv_filename = filename.replace('.xlsx', '.csv')
            error_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"ğŸ“‹ ÄÃ£ lÆ°u error log CSV: {csv_filename}")
            return csv_filename

    def retry_failed_requests(self):
        """Thá»­ láº¡i cÃ¡c request bá»‹ lá»—i"""
        if not self.error_log:
            print("ğŸ“‹ KhÃ´ng cÃ³ lá»—i nÃ o Ä‘á»ƒ retry")
            return []
        
        print(f"\nğŸ”„ === Báº®T Äáº¦U RETRY {len(self.error_log)} Lá»–I ===")
        
        retry_data = []
        success_retry = 0
        
        # Láº¥y danh sÃ¡ch unique URLs cáº§n retry
        unique_errors = {}
        for error in self.error_log:
            key = (error['ma_tinh'], error['ma_xa'])
            if key not in unique_errors:
                unique_errors[key] = error
        
        for i, (key, error) in enumerate(unique_errors.items(), 1):
            ma_tinh = error['ma_tinh']
            ma_xa = error['ma_xa']
            ten_tinh = error['ten_tinh'] or f"Tá»‰nh {ma_tinh}"
            ten_xa = error['ten_xa'] or f"XÃ£ {ma_xa}"
            
            print(f"\nğŸ”„ [{i}/{len(unique_errors)}] Retry: {ten_xa}")
            
            # Thá»­ láº¡i vá»›i delay lá»›n hÆ¡n
            time.sleep(2)
            
            xa_info = self.get_sap_nhap_details(ma_tinh, ma_xa, ten_tinh, ten_xa)
            
            if xa_info:
                retry_data.append(xa_info)
                success_retry += 1
                print(f"    âœ… Retry thÃ nh cÃ´ng!")
            else:
                print(f"    âŒ Retry váº«n lá»—i")
        
        print(f"\nğŸ‰ RETRY HOÃ€N THÃ€NH!")
        print(f"ğŸ“Š ThÃ nh cÃ´ng: {success_retry}/{len(unique_errors)}")
        
        return retry_data

    def print_statistics(self):
        """In thá»‘ng kÃª chi tiáº¿t"""
        print(f"\nğŸ“Š === THá»NG KÃŠ CHI TIáº¾T ===")
        print(f"ğŸ“ˆ Tá»•ng sá»‘ request: {self.stats['total_processed']}")
        print(f"âœ… ThÃ nh cÃ´ng: {self.stats['success_count']}")
        print(f"âŒ Lá»—i: {self.stats['error_count']}")
        
        if self.stats['error_count'] > 0:
            print(f"\nğŸ“‹ Chi tiáº¿t lá»—i:")
            print(f"  ğŸš« Rate limit: {self.stats['rate_limit_count']}")
            print(f"  â° Timeout: {self.stats['timeout_count']}")
            print(f"  ğŸ”Œ Connection error: {self.stats['connection_error_count']}")
            
            success_rate = (self.stats['success_count'] / self.stats['total_processed']) * 100
            print(f"\nğŸ“ˆ Tá»· lá»‡ thÃ nh cÃ´ng: {success_rate:.1f}%")
        
        if self.error_log:
            print(f"\nğŸ’¡ Äá» xuáº¥t:")
            print(f"  - CÃ³ {len(self.error_log)} lá»—i cáº§n xá»­ lÃ½ láº¡i")
            print(f"  - Cháº¡y crawler.retry_failed_requests() Ä‘á»ƒ thá»­ láº¡i")
            print(f"  - Cháº¡y crawler.save_error_log() Ä‘á»ƒ lÆ°u danh sÃ¡ch lá»—i")

    def crawl_known_data(self, max_items=None):
        """KÃ©o dá»¯ liá»‡u tá»« danh sÃ¡ch Ä‘Ã£ biáº¿t"""
        print("ğŸš€ Báº¯t Ä‘áº§u kÃ©o dá»¯ liá»‡u tá»« danh sÃ¡ch Ä‘Ã£ biáº¿t...")
        
        # Láº¥y thÃ´ng tin cáº¥p tá»‰nh trÆ°á»›c
        print("\nğŸ›ï¸  === THÃ”NG TIN Cáº¤P Tá»ˆNH ===")
        for province in self.provinces:
            ma_tinh = province['ma_tinh']
            ten_tinh = province['ten_tinh']
            
            print(f"\nğŸ“ {ten_tinh} (MÃ£: {ma_tinh})")
            tinh_info = self.get_sap_nhap_details(ma_tinh, ten_tinh=ten_tinh)
            
            if tinh_info:
                self.data.append(tinh_info)
        
        # Láº¥y thÃ´ng tin cáº¥p xÃ£/phÆ°á»ng
        print(f"\nğŸ˜ï¸  === THÃ”NG TIN Cáº¤P XÃƒ/PHÆ¯á»œNG ===")
        
        items_to_process = self.known_data[:max_items] if max_items else self.known_data
        
        for i, item in enumerate(items_to_process, 1):
            ma_tinh = item['ma_tinh']
            ten_tinh = item['ten_tinh']
            ma_xa = item['ma_xa']
            ten_xa = item['ten_xa']
            
            print(f"\nğŸ“ [{i}/{len(items_to_process)}] {ten_xa}")
            
            xa_info = self.get_sap_nhap_details(ma_tinh, ma_xa, ten_tinh, ten_xa)
            
            if xa_info:
                self.data.append(xa_info)
            
            time.sleep(0.5)  # Delay Ä‘á»ƒ trÃ¡nh bá»‹ block
        
        print(f"\nğŸ‰ HoÃ n thÃ nh! Tá»•ng cá»™ng: {len(self.data)} báº£n ghi")
        return self.data
    
    def save_to_excel(self, filename=None):
        """LÆ°u dá»¯ liá»‡u ra file Excel"""
        if not self.data:
            print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u")
            return
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"sap_nhap_simple_{timestamp}.xlsx"
        
        df = pd.DataFrame(self.data)
        
        # LÃ m sáº¡ch dá»¯ liá»‡u
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].astype(str).apply(lambda x: re.sub(r'[^\x20-\x7E\u00A0-\uFFFF]', '', str(x)))
                df[col] = df[col].apply(lambda x: x[:32767] if len(str(x)) > 32767 else x)
                df[col] = df[col].fillna('')
        
        try:
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # Sheet dá»¯ liá»‡u chÃ­nh
                df.to_excel(writer, sheet_name='Dá»¯ liá»‡u sÃ¡p nháº­p', index=False)
                
                # Sheet thá»‘ng kÃª
                stats = []
                
                if 'cap_hanh_chinh' in df.columns:
                    cap_stats = df['cap_hanh_chinh'].value_counts()
                    for cap, count in cap_stats.items():
                        stats.append({'Loáº¡i': 'Cáº¥p hÃ nh chÃ­nh', 'GiÃ¡ trá»‹': cap, 'Sá»‘ lÆ°á»£ng': count})
                
                if 'co_thong_tin' in df.columns:
                    info_stats = df['co_thong_tin'].value_counts()
                    for has_info, count in info_stats.items():
                        label = 'CÃ³ thÃ´ng tin sÃ¡p nháº­p' if has_info else 'KhÃ´ng cÃ³ thÃ´ng tin'
                        stats.append({'Loáº¡i': 'ThÃ´ng tin sÃ¡p nháº­p', 'GiÃ¡ trá»‹': label, 'Sá»‘ lÆ°á»£ng': count})
                
                if 'so_luong_thay_doi' in df.columns:
                    total_changes = df['so_luong_thay_doi'].sum()
                    stats.append({'Loáº¡i': 'Tá»•ng sá»‘ thay Ä‘á»•i', 'GiÃ¡ trá»‹': '', 'Sá»‘ lÆ°á»£ng': total_changes})
                
                # Thá»‘ng kÃª crawling
                stats.append({'Loáº¡i': 'Tá»•ng request', 'GiÃ¡ trá»‹': '', 'Sá»‘ lÆ°á»£ng': self.stats['total_processed']})
                stats.append({'Loáº¡i': 'Request thÃ nh cÃ´ng', 'GiÃ¡ trá»‹': '', 'Sá»‘ lÆ°á»£ng': self.stats['success_count']})
                stats.append({'Loáº¡i': 'Request lá»—i', 'GiÃ¡ trá»‹': '', 'Sá»‘ lÆ°á»£ng': self.stats['error_count']})
                
                if self.stats['total_processed'] > 0:
                    success_rate = (self.stats['success_count'] / self.stats['total_processed']) * 100
                    stats.append({'Loáº¡i': 'Tá»· lá»‡ thÃ nh cÃ´ng (%)', 'GiÃ¡ trá»‹': '', 'Sá»‘ lÆ°á»£ng': round(success_rate, 1)})
                
                # Chi tiáº¿t lá»—i
                if self.stats['error_count'] > 0:
                    stats.append({'Loáº¡i': 'Rate limit errors', 'GiÃ¡ trá»‹': '', 'Sá»‘ lÆ°á»£ng': self.stats['rate_limit_count']})
                    stats.append({'Loáº¡i': 'Timeout errors', 'GiÃ¡ trá»‹': '', 'Sá»‘ lÆ°á»£ng': self.stats['timeout_count']})
                    stats.append({'Loáº¡i': 'Connection errors', 'GiÃ¡ trá»‹': '', 'Sá»‘ lÆ°á»£ng': self.stats['connection_error_count']})
                
                if stats:
                    stats_df = pd.DataFrame(stats)
                    stats_df.to_excel(writer, sheet_name='Thá»‘ng kÃª', index=False)
                
                # Sheet dá»¯ liá»‡u cÃ³ thÃ´ng tin sÃ¡p nháº­p
                if 'co_thong_tin' in df.columns:
                    df_with_info = df[df['co_thong_tin'] == True]
                    if len(df_with_info) > 0:
                        df_with_info.to_excel(writer, sheet_name='CÃ³ thÃ´ng tin sÃ¡p nháº­p', index=False)
                
                # Sheet lá»—i náº¿u cÃ³
                if self.error_log:
                    error_df = pd.DataFrame(self.error_log)
                    error_df.to_excel(writer, sheet_name='Danh sÃ¡ch lá»—i', index=False)
            
            print(f"ğŸ’¾ ÄÃ£ lÆ°u: {filename}")
            
            # In thá»‘ng kÃª
            self.print_statistics()
            
            # LÆ°u error log riÃªng náº¿u cÃ³ lá»—i
            if self.error_log:
                self.save_error_log()
            
        except Exception as e:
            print(f"âŒ Lá»—i lÆ°u Excel: {e}")
            # Fallback CSV
            csv_filename = filename.replace('.xlsx', '.csv')
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ ÄÃ£ lÆ°u CSV: {csv_filename}")
            filename = csv_filename
        
        return filename

def main():
    """HÃ m chÃ­nh"""
    print("=== TOOL KÃ‰O Dá»® LIá»†U Äá»ŠA CHá»ˆ SÃP NHáº¬P - PHIÃŠN Báº¢N ÄÆ N GIáº¢N ===")
    print("ğŸŒ Website: https://thuvienphapluat.vn")
    print("ğŸ¯ Chiáº¿n lÆ°á»£c: Auto-discovery hoáº·c dá»¯ liá»‡u máº«u")
    print("=" * 70)
    
    print("\nğŸ“‹ TÃ¹y chá»n:")
    print("1. ğŸ§ª Test vá»›i 5 Ä‘á»‹a chá»‰ máº«u")
    print("2. ğŸ“Š KÃ©o táº¥t cáº£ dá»¯ liá»‡u máº«u cÃ³ sáºµn")
    print("3. ğŸŒ Auto-discovery: Tá»± Ä‘á»™ng tÃ¬m táº¥t cáº£ tá»‰nh/xÃ£")
    print("4. ğŸ” Auto-discovery: Chá»‰ tÃ¬m má»™t sá»‘ tá»‰nh Ä‘áº§u tiÃªn")
    print("5. ğŸ”„ Retry cÃ¡c lá»—i tá»« láº§n crawl trÆ°á»›c")
    
    try:
        choice = input("\nâ¤ Nháº­p lá»±a chá»n (1-5): ").strip()
        
        crawler = SapNhapCrawlerSimple()
        
        if choice == "1":
            data = crawler.crawl_known_data(max_items=5)
            
        elif choice == "2":
            data = crawler.crawl_known_data()
            
        elif choice == "3":
            # Auto-discovery táº¥t cáº£
            print("\nğŸš€ Báº¯t Ä‘áº§u auto-discovery toÃ n bá»™...")
            data = crawler.crawl_all_autodiscovery()
            
        elif choice == "4":
            # Auto-discovery cÃ³ giá»›i háº¡n
            limit = input("â¤ Nháº­p sá»‘ tá»‰nh muá»‘n crawl (máº·c Ä‘á»‹nh 5): ").strip()
            try:
                limit = int(limit) if limit else 5
            except:
                limit = 5
            print(f"\nğŸš€ Báº¯t Ä‘áº§u auto-discovery {limit} tá»‰nh Ä‘áº§u tiÃªn...")
            data = crawler.crawl_all_autodiscovery(max_provinces=limit)
            
        elif choice == "5":
            # Retry cÃ¡c lá»—i
            print("\nğŸ”„ Báº¯t Ä‘áº§u retry cÃ¡c lá»—i tá»« láº§n crawl trÆ°á»›c...")
            print("âš ï¸  Chá»©c nÄƒng nÃ y cáº§n cÃ³ file error log tá»« láº§n crawl trÆ°á»›c")
            
            # Load error log tá»« file gáº§n nháº¥t
            error_files = [f for f in os.listdir('.') if f.startswith('error_log_') and f.endswith('.xlsx')]
            if error_files:
                latest_error_file = max(error_files, key=lambda x: os.path.getctime(x))
                print(f"ğŸ“‹ TÃ¬m tháº¥y error log: {latest_error_file}")
                
                try:
                    error_df = pd.read_excel(latest_error_file, sheet_name='Danh sÃ¡ch lá»—i')
                    crawler.error_log = error_df.to_dict('records')
                    print(f"ğŸ“‹ Loaded {len(crawler.error_log)} lá»—i Ä‘á»ƒ retry")
                    
                    data = crawler.retry_failed_requests()
                except Exception as e:
                    print(f"âŒ Lá»—i load error log: {e}")
                    data = []
            else:
                print("âŒ KhÃ´ng tÃ¬m tháº¥y file error log nÃ o")
                data = []
            
        else:
            print("âš ï¸  Lá»±a chá»n khÃ´ng há»£p lá»‡. Cháº¡y test máº·c Ä‘á»‹nh.")
            data = crawler.crawl_known_data(max_items=5)
        
        # LÆ°u káº¿t quáº£
        if data:
            filename = crawler.save_to_excel()
            
            print(f"\nğŸ“Š === Káº¾T QUáº¢ CUá»I CÃ™NG ===")
            df = pd.DataFrame(data)
            
            print(f"ğŸ“ˆ Tá»•ng sá»‘ báº£n ghi: {len(df)}")
            
            if 'cap_hanh_chinh' in df.columns:
                print(f"\nğŸ›ï¸  PhÃ¢n loáº¡i theo cáº¥p:")
                for cap, count in df['cap_hanh_chinh'].value_counts().items():
                    print(f"   {cap}: {count}")
            
            if 'co_thong_tin' in df.columns:
                info_count = df[df['co_thong_tin'] == True]
                print(f"\nâœ… CÃ³ thÃ´ng tin sÃ¡p nháº­p: {len(info_count)}")
                no_info_count = df[df['co_thong_tin'] == False]
                print(f"âšª KhÃ´ng cÃ³ thÃ´ng tin: {len(no_info_count)}")
            
            if 'so_luong_thay_doi' in df.columns:
                total_changes = df['so_luong_thay_doi'].sum()
                print(f"ğŸ”„ Tá»•ng sá»‘ thay Ä‘á»•i: {total_changes}")
            
            print(f"\nğŸ’¾ File káº¿t quáº£: {filename}")
            print(f"\nğŸ‰ HOÃ€N THÃ€NH THÃ€NH CÃ”NG!")
            
        else:
            print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u nÃ o Ä‘Æ°á»£c kÃ©o vá»")
            
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ÄÃ£ dá»«ng theo yÃªu cáº§u ngÆ°á»i dÃ¹ng")
        if 'crawler' in locals() and crawler.data:
            print("ğŸ’¾ Äang lÆ°u dá»¯ liá»‡u Ä‘Ã£ kÃ©o Ä‘Æ°á»£c...")
            crawler.save_to_excel()
            
    except Exception as e:
        print(f"\nâŒ Lá»—i: {e}")
        import traceback
        traceback.print_exc()
        
        if 'crawler' in locals() and crawler.data:
            print("ğŸ’¾ Äang lÆ°u dá»¯ liá»‡u Ä‘Ã£ kÃ©o Ä‘Æ°á»£c...")
            crawler.save_to_excel()

if __name__ == "__main__":
    main()
