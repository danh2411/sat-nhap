#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script FINAL ƒë·ªÉ k√©o d·ªØ li·ªáu ƒë·ªãa ch·ªâ s√°p nh·∫≠p t·ª´ ThuvienPhapluat.vn
Phi√™n b·∫£n cu·ªëi c√πng - T·ªëi ∆∞u h√≥a v√† ho√†n thi·ªán
"""

import requests
import json
import pandas as pd
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, parse_qs, urlparse
import re
import os
from datetime import datetime
import html
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

class SapNhapCrawlerFinal:
    def __init__(self, max_workers=3):
        self.base_url = "https://thuvienphapluat.vn"
        self.search_url = "https://thuvienphapluat.vn/ma-so-thue/tra-cuu-thong-tin-sap-nhap-tinh"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.data = []
        self.provinces = []
        self.failed_requests = []
        self.max_workers = max_workers
        self.lock = threading.Lock()
        
        # Danh s√°ch t·ªânh/th√†nh c·ªë ƒë·ªãnh (fallback)
        self.fallback_provinces = [
            {'ma_tinh': '01', 'ten_tinh': 'H√† N·ªôi'},
            {'ma_tinh': '02', 'ten_tinh': 'H√† Giang'},
            {'ma_tinh': '04', 'ten_tinh': 'Cao B·∫±ng'},
            {'ma_tinh': '06', 'ten_tinh': 'B·∫Øc K·∫°n'},
            {'ma_tinh': '08', 'ten_tinh': 'Tuy√™n Quang'},
            {'ma_tinh': '10', 'ten_tinh': 'L√†o Cai'},
            {'ma_tinh': '11', 'ten_tinh': 'ƒêi·ªán Bi√™n'},
            {'ma_tinh': '12', 'ten_tinh': 'Lai Ch√¢u'},
            {'ma_tinh': '14', 'ten_tinh': 'S∆°n La'},
            {'ma_tinh': '15', 'ten_tinh': 'Y√™n B√°i'},
            {'ma_tinh': '17', 'ten_tinh': 'Ho√† B√¨nh'},
            {'ma_tinh': '19', 'ten_tinh': 'Th√°i Nguy√™n'},
            {'ma_tinh': '20', 'ten_tinh': 'L·∫°ng S∆°n'},
            {'ma_tinh': '22', 'ten_tinh': 'Qu·∫£ng Ninh'},
            {'ma_tinh': '24', 'ten_tinh': 'B·∫Øc Giang'},
            {'ma_tinh': '25', 'ten_tinh': 'Ph√∫ Th·ªç'},
            {'ma_tinh': '26', 'ten_tinh': 'Vƒ©nh Ph√∫c'},
            {'ma_tinh': '27', 'ten_tinh': 'B·∫Øc Ninh'},
            {'ma_tinh': '30', 'ten_tinh': 'H·∫£i D∆∞∆°ng'},
            {'ma_tinh': '31', 'ten_tinh': 'H·∫£i Ph√≤ng'},
            {'ma_tinh': '33', 'ten_tinh': 'H∆∞ng Y√™n'},
            {'ma_tinh': '34', 'ten_tinh': 'Th√°i B√¨nh'},
            {'ma_tinh': '35', 'ten_tinh': 'H√† Nam'},
            {'ma_tinh': '36', 'ten_tinh': 'Nam ƒê·ªãnh'},
            {'ma_tinh': '37', 'ten_tinh': 'Ninh B√¨nh'},
            {'ma_tinh': '38', 'ten_tinh': 'Thanh H√≥a'},
            {'ma_tinh': '40', 'ten_tinh': 'Ngh·ªá An'},
            {'ma_tinh': '42', 'ten_tinh': 'H√† Tƒ©nh'},
            {'ma_tinh': '44', 'ten_tinh': 'Qu·∫£ng B√¨nh'},
            {'ma_tinh': '45', 'ten_tinh': 'Qu·∫£ng Tr·ªã'},
            {'ma_tinh': '46', 'ten_tinh': 'Th·ª´a Thi√™n Hu·∫ø'},
            {'ma_tinh': '48', 'ten_tinh': 'ƒê√† N·∫µng'},
            {'ma_tinh': '49', 'ten_tinh': 'Qu·∫£ng Nam'},
            {'ma_tinh': '51', 'ten_tinh': 'Qu·∫£ng Ng√£i'},
            {'ma_tinh': '52', 'ten_tinh': 'B√¨nh ƒê·ªãnh'},
            {'ma_tinh': '54', 'ten_tinh': 'Ph√∫ Y√™n'},
            {'ma_tinh': '56', 'ten_tinh': 'Kh√°nh H√≤a'},
            {'ma_tinh': '58', 'ten_tinh': 'Ninh Thu·∫≠n'},
            {'ma_tinh': '60', 'ten_tinh': 'B√¨nh Thu·∫≠n'},
            {'ma_tinh': '62', 'ten_tinh': 'Kon Tum'},
            {'ma_tinh': '64', 'ten_tinh': 'Gia Lai'},
            {'ma_tinh': '66', 'ten_tinh': 'ƒê·∫Øk L·∫Øk'},
            {'ma_tinh': '67', 'ten_tinh': 'ƒê·∫Øk N√¥ng'},
            {'ma_tinh': '68', 'ten_tinh': 'L√¢m ƒê·ªìng'},
            {'ma_tinh': '70', 'ten_tinh': 'B√¨nh Ph∆∞·ªõc'},
            {'ma_tinh': '72', 'ten_tinh': 'T√¢y Ninh'},
            {'ma_tinh': '74', 'ten_tinh': 'B√¨nh D∆∞∆°ng'},
            {'ma_tinh': '75', 'ten_tinh': 'ƒê·ªìng Nai'},
            {'ma_tinh': '77', 'ten_tinh': 'B√† R·ªãa - V≈©ng T√†u'},
            {'ma_tinh': '79', 'ten_tinh': 'H·ªì Ch√≠ Minh'},
            {'ma_tinh': '80', 'ten_tinh': 'Long An'},
            {'ma_tinh': '82', 'ten_tinh': 'Ti·ªÅn Giang'},
            {'ma_tinh': '83', 'ten_tinh': 'Vƒ©nh Long'},
            {'ma_tinh': '84', 'ten_tinh': 'Tr√† Vinh'},
            {'ma_tinh': '86', 'ten_tinh': 'ƒê·ªìng Th√°p'},
            {'ma_tinh': '87', 'ten_tinh': 'An Giang'},
            {'ma_tinh': '89', 'ten_tinh': 'Ki√™n Giang'},
            {'ma_tinh': '91', 'ten_tinh': 'C√† Mau'},
            {'ma_tinh': '92', 'ten_tinh': 'C·∫ßn Th∆°'},
            {'ma_tinh': '93', 'ten_tinh': 'H·∫≠u Giang'},
            {'ma_tinh': '94', 'ten_tinh': 'S√≥c TrƒÉng'},
            {'ma_tinh': '95', 'ten_tinh': 'B·∫°c Li√™u'},
            {'ma_tinh': '96', 'ten_tinh': 'C√† Mau'}
        ]
        
    def get_page_content(self, url):
        """L·∫•y n·ªôi dung trang web v·ªõi retry mechanism"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except requests.RequestException as e:
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    with self.lock:
                        self.failed_requests.append({'url': url, 'error': str(e)})
                    return None
    
    def extract_xa_phuong_from_links(self, soup, ma_tinh):
        """Tr√≠ch xu·∫•t danh s√°ch x√£/ph∆∞·ªùng t·ª´ c√°c link trong trang"""
        xa_phuong_list = []
        seen_xa = set()
        
        # T√¨m t·∫•t c·∫£ c√°c link c√≥ pattern MaXa (kh√¥ng c·∫ßn kh·ªõp MaTinh trong regex)
        links = soup.find_all('a', href=re.compile(r'MaXa=\d+'))
        
        for link in links:
            href = link.get('href')
            if href and f'MaTinh={ma_tinh}' in href:  # Ki·ªÉm tra MaTinh trong href
                parsed = urlparse(href)
                query_params = parse_qs(parsed.query)
                if 'MaXa' in query_params:
                    ma_xa = query_params['MaXa'][0]
                    if ma_xa not in seen_xa:
                        seen_xa.add(ma_xa)
                        text = link.text.strip()
                        # L√†m s·∫°ch text
                        text = re.sub(r'^\d+\.\s*', '', text)  # B·ªè s·ªë th·ª© t·ª±
                        xa_phuong_list.append({
                            'ma_xa': ma_xa,
                            'ten_xa': text
                        })
        
        return xa_phuong_list
    
    def parse_sap_nhap_info(self, soup):
        """Ph√¢n t√≠ch th√¥ng tin s√°p nh·∫≠p t·ª´ trang"""
        info = {
            'truoc_sap_nhap': '',
            'sau_sap_nhap': '',
            'chi_tiet': [],
            'raw_text': ''
        }
        
        # L·∫•y to√†n b·ªô text ƒë·ªÉ backup
        full_text = soup.get_text(separator=' ', strip=True)
        info['raw_text'] = full_text[:1000]  # Gi·ªõi h·∫°n 1000 k√Ω t·ª±
        
        # T√¨m b·∫£ng c√≥ th√¥ng tin s√°p nh·∫≠p
        tables = soup.find_all('table')
        
        for table in tables:
            rows = table.find_all('tr')
            
            # T√¨m header row
            for i, row in enumerate(rows):
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    cell_texts = [cell.get_text(strip=True).lower() for cell in cells]
                    
                    # Ki·ªÉm tra n·∫øu c√≥ header tr∆∞·ªõc/sau s√°p nh·∫≠p
                    has_truoc = any('tr∆∞·ªõc s√°p nh·∫≠p' in text for text in cell_texts)
                    has_sau = any('sau s√°p nh·∫≠p' in text for text in cell_texts)
                    
                    if has_truoc and has_sau:
                        # T√¨m v·ªã tr√≠ c·ªôt
                        truoc_col = next((j for j, text in enumerate(cell_texts) if 'tr∆∞·ªõc s√°p nh·∫≠p' in text), 0)
                        sau_col = next((j for j, text in enumerate(cell_texts) if 'sau s√°p nh·∫≠p' in text), 1)
                        
                        # L·∫•y d·ªØ li·ªáu t·ª´ c√°c row ti·∫øp theo
                        for j in range(i + 1, len(rows)):
                            data_row = rows[j]
                            data_cells = data_row.find_all(['td', 'th'])
                            
                            if len(data_cells) > max(truoc_col, sau_col):
                                truoc_text = html.unescape(data_cells[truoc_col].get_text(strip=True))
                                sau_text = html.unescape(data_cells[sau_col].get_text(strip=True))
                                
                                if truoc_text and sau_text:
                                    info['chi_tiet'].append({
                                        'truoc': truoc_text,
                                        'sau': sau_text
                                    })
                                    
                                    # L·∫•y th√¥ng tin t·ªïng quan (d√≤ng ƒë·∫ßu ti√™n)
                                    if not info['truoc_sap_nhap']:
                                        info['truoc_sap_nhap'] = truoc_text
                                    if not info['sau_sap_nhap']:
                                        info['sau_sap_nhap'] = sau_text
                        break
        
        # N·∫øu kh√¥ng t√¨m th·∫•y trong b·∫£ng, th·ª≠ t√¨m trong text
        if not info['truoc_sap_nhap'] and not info['sau_sap_nhap']:
            # Pattern ƒë∆°n gi·∫£n ƒë·ªÉ t√¨m th√¥ng tin
            patterns = [
                (r'tr∆∞·ªõc s√°p nh·∫≠p[:\s]*([^\n]+)', 'truoc_sap_nhap'),
                (r'sau s√°p nh·∫≠p[:\s]*([^\n]+)', 'sau_sap_nhap'),
            ]
            
            for pattern, key in patterns:
                match = re.search(pattern, full_text, re.IGNORECASE)
                if match:
                    info[key] = match.group(1).strip()
        
        return info
    
    def get_xa_phuong_for_province(self, ma_tinh):
        """L·∫•y danh s√°ch x√£/ph∆∞·ªùng cho m·ªôt t·ªânh"""
        url = f"{self.search_url}?MaTinh={ma_tinh}"
        content = self.get_page_content(url)
        
        if not content:
            print(f"    Kh√¥ng th·ªÉ l·∫•y n·ªôi dung t·ª´ {url}")
            return []
        
        soup = BeautifulSoup(content, 'html.parser')
        
        # Debug: ƒê·∫øm t·∫•t c·∫£ link c√≥ MaXa
        all_xa_links = soup.find_all('a', href=re.compile(r'MaXa=\d+'))
        print(f"    Debug: T√¨m th·∫•y {len(all_xa_links)} link c√≥ MaXa")
        
        # Debug: ƒê·∫øm link c√≥ MaTinh ph√π h·ª£p
        relevant_links = [link for link in all_xa_links if f'MaTinh={ma_tinh}' in link.get('href', '')]
        print(f"    Debug: Trong ƒë√≥ {len(relevant_links)} link thu·ªôc t·ªânh {ma_tinh}")
        
        xa_phuong_list = self.extract_xa_phuong_from_links(soup, ma_tinh)
        
        if not xa_phuong_list:
            print(f"    Debug: Kh√¥ng t√¨m th·∫•y x√£/ph∆∞·ªùng, c√≥ th·ªÉ do logic extract sai")
            # Th·ª≠ manual
            if relevant_links:
                print(f"    Debug: Th·ª≠ extract manual t·ª´ {len(relevant_links)} link ƒë·∫ßu ti√™n")
                for link in relevant_links[:3]:
                    href = link.get('href', '')
                    text = link.text.strip()
                    print(f"      Link: {href} -> {text}")
        
        return xa_phuong_list
    
    def get_sap_nhap_details(self, ma_tinh, ma_xa=None, ten_tinh='', ten_xa=''):
        """L·∫•y chi ti·∫øt th√¥ng tin s√°p nh·∫≠p"""
        if ma_xa:
            url = f"{self.search_url}?MaTinh={ma_tinh}&MaXa={ma_xa}"
            cap_hanh_chinh = 'X√£/Ph∆∞·ªùng'
        else:
            url = f"{self.search_url}?MaTinh={ma_tinh}"
            cap_hanh_chinh = 'T·ªânh/Th√†nh ph·ªë'
        
        content = self.get_page_content(url)
        if not content:
            return None
        
        soup = BeautifulSoup(content, 'html.parser')
        sap_nhap_info = self.parse_sap_nhap_info(soup)
        
        result = {
            'ma_tinh': ma_tinh,
            'ten_tinh': ten_tinh,
            'ma_xa': ma_xa or '',
            'ten_xa': ten_xa,
            'cap_hanh_chinh': cap_hanh_chinh,
            'url': url,
            'truoc_sap_nhap': sap_nhap_info['truoc_sap_nhap'],
            'sau_sap_nhap': sap_nhap_info['sau_sap_nhap'],
            'chi_tiet_json': json.dumps(sap_nhap_info['chi_tiet'], ensure_ascii=False) if sap_nhap_info['chi_tiet'] else '',
            'so_luong_thay_doi': len(sap_nhap_info['chi_tiet']),
            'raw_text_sample': sap_nhap_info['raw_text']
        }
        
        return result
    
    def process_province(self, province, max_xa=None):
        """X·ª≠ l√Ω m·ªôt t·ªânh (c√≥ th·ªÉ ch·∫°y song song)"""
        ma_tinh = province['ma_tinh']
        ten_tinh = province['ten_tinh']
        
        province_data = []
        
        print(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {ten_tinh} (M√£: {ma_tinh})")
        
        # L·∫•y th√¥ng tin c·∫•p t·ªânh
        tinh_info = self.get_sap_nhap_details(ma_tinh, ten_tinh=ten_tinh)
        if tinh_info:
            province_data.append(tinh_info)
        
        # L·∫•y danh s√°ch x√£/ph∆∞·ªùng
        xa_phuong_list = self.get_xa_phuong_for_province(ma_tinh)
        print(f"{ten_tinh}: T√¨m th·∫•y {len(xa_phuong_list)} x√£/ph∆∞·ªùng")
        
        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng x√£ n·∫øu c·∫ßn
        if max_xa and len(xa_phuong_list) > max_xa:
            xa_phuong_list = xa_phuong_list[:max_xa]
            print(f"{ten_tinh}: Gi·ªõi h·∫°n {max_xa} x√£ ƒë·∫ßu ti√™n")
        
        # X·ª≠ l√Ω t·ª´ng x√£/ph∆∞·ªùng
        for xa_phuong in xa_phuong_list:
            ma_xa = xa_phuong['ma_xa']
            ten_xa = xa_phuong['ten_xa']
            
            xa_info = self.get_sap_nhap_details(ma_tinh, ma_xa, ten_tinh, ten_xa)
            if xa_info:
                province_data.append(xa_info)
            
            time.sleep(0.2)  # Delay nh·ªè
        
        print(f"Ho√†n th√†nh {ten_tinh}: {len(province_data)} b·∫£n ghi")
        return province_data
    
    def crawl_sample_data(self, max_provinces=3, max_xa_per_province=5):
        """K√©o d·ªØ li·ªáu m·∫´u"""
        print(f"K√©o d·ªØ li·ªáu m·∫´u: {max_provinces} t·ªânh, t·ªëi ƒëa {max_xa_per_province} x√£/t·ªânh")
        
        # S·ª≠ d·ª•ng danh s√°ch fallback v·ªõi t·ªânh c√≥ d·ªØ li·ªáu
        sample_provinces = [
            {'ma_tinh': '83', 'ten_tinh': 'Vƒ©nh Long'},
            {'ma_tinh': '79', 'ten_tinh': 'H·ªì Ch√≠ Minh'},
            {'ma_tinh': '01', 'ten_tinh': 'H√† N·ªôi'}
        ][:max_provinces]
        
        for province in sample_provinces:
            province_data = self.process_province(province, max_xa_per_province)
            with self.lock:
                self.data.extend(province_data)
        
        print(f"Ho√†n th√†nh m·∫´u! T·ªïng: {len(self.data)} b·∫£n ghi")
        return self.data
    
    def crawl_full_data(self, use_threading=True):
        """K√©o to√†n b·ªô d·ªØ li·ªáu"""
        print("K√©o TO√ÄN B·ªò d·ªØ li·ªáu...")
        
        provinces = self.fallback_provinces
        
        if use_threading and self.max_workers > 1:
            print(f"S·ª≠ d·ª•ng {self.max_workers} threads")
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = [executor.submit(self.process_province, province) for province in provinces]
                
                for i, future in enumerate(as_completed(futures)):
                    try:
                        province_data = future.result()
                        with self.lock:
                            self.data.extend(province_data)
                        print(f"Ho√†n th√†nh {i+1}/{len(provinces)} t·ªânh")
                    except Exception as e:
                        print(f"L·ªói khi x·ª≠ l√Ω t·ªânh: {e}")
        else:
            # X·ª≠ l√Ω tu·∫ßn t·ª±
            for i, province in enumerate(provinces, 1):
                print(f"[{i}/{len(provinces)}] ", end='')
                province_data = self.process_province(province)
                self.data.extend(province_data)
                
                # Checkpoint m·ªói 10 t·ªânh
                if i % 10 == 0:
                    self.save_checkpoint(f"checkpoint_{i}")
        
        print(f"HO√ÄN TH√ÄNH! T·ªïng: {len(self.data)} b·∫£n ghi")
        return self.data
    
    def save_checkpoint(self, filename_prefix):
        """L∆∞u checkpoint"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename_prefix}_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
        
        print(f"Checkpoint: {filename}")
    
    def clean_data_for_excel(self, df):
        """L√†m s·∫°ch d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u Excel"""
        import re
        
        for col in df.columns:
            if df[col].dtype == 'object':  # Text columns
                # Lo·∫°i b·ªè k√Ω t·ª± kh√¥ng h·ª£p l·ªá
                df[col] = df[col].astype(str).apply(lambda x: re.sub(r'[^\x20-\x7E\u00A0-\uFFFF]', '', str(x)) if pd.notna(x) else '')
                # Gi·ªõi h·∫°n ƒë·ªô d√†i chu·ªói
                df[col] = df[col].apply(lambda x: x[:32767] if len(str(x)) > 32767 else x)
                # Thay th·∫ø null
                df[col] = df[col].fillna('')
        
        return df
    
    def save_to_excel(self, filename=None):
        """L∆∞u d·ªØ li·ªáu ra file Excel"""
        if not self.data:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u")
            return
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"sap_nhap_final_{timestamp}.xlsx"
        
        df = pd.DataFrame(self.data)
        
        # L√†m s·∫°ch d·ªØ li·ªáu
        df = self.clean_data_for_excel(df)
        
        try:
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # Sheet ch√≠nh
                df.to_excel(writer, sheet_name='D·ªØ li·ªáu s√°p nh·∫≠p', index=False)
                
                # Sheet th·ªëng k√™
                stats = []
                if 'cap_hanh_chinh' in df.columns:
                    cap_stats = df['cap_hanh_chinh'].value_counts()
                    for cap, count in cap_stats.items():
                        stats.append({'Lo·∫°i': 'C·∫•p h√†nh ch√≠nh', 'Gi√° tr·ªã': cap, 'S·ªë l∆∞·ª£ng': count})
                
                if 'ten_tinh' in df.columns:
                    tinh_count = df['ten_tinh'].nunique()
                    stats.append({'Lo·∫°i': 'T·ªïng s·ªë t·ªânh/th√†nh', 'Gi√° tr·ªã': '', 'S·ªë l∆∞·ª£ng': tinh_count})
                
                if 'so_luong_thay_doi' in df.columns:
                    total_changes = df['so_luong_thay_doi'].sum()
                    stats.append({'Lo·∫°i': 'T·ªïng s·ªë thay ƒë·ªïi', 'Gi√° tr·ªã': '', 'S·ªë l∆∞·ª£ng': total_changes})
                
                if stats:
                    stats_df = pd.DataFrame(stats)
                    stats_df.to_excel(writer, sheet_name='Th·ªëng k√™', index=False)
                
                # Sheet l·ªói
                if self.failed_requests:
                    error_df = pd.DataFrame(self.failed_requests)
                    error_df.to_excel(writer, sheet_name='L·ªói request', index=False)
            
            print(f"ƒê√£ l∆∞u: {filename}")
            print(f"T·ªïng s·ªë b·∫£n ghi: {len(df)}")
            
        except Exception as e:
            print(f"L·ªói khi l∆∞u Excel: {e}")
            # Fallback: l∆∞u CSV
            csv_filename = filename.replace('.xlsx', '.csv')
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"ƒê√£ l∆∞u CSV thay th·∫ø: {csv_filename}")
            filename = csv_filename
        
        return filename

def main():
    """H√†m ch√≠nh"""
    print("=== TOOL K√âO D·ªÆ LI·ªÜU ƒê·ªäA CH·ªà S√ÅP NH·∫¨P - PHI√äN B·∫¢N CU·ªêI C√ôNG ===")
    print("Website: https://thuvienphapluat.vn")
    print("T√°c nƒÉng: K√©o to√†n b·ªô d·ªØ li·ªáu ƒë·ªãa ch·ªâ m·ªõi v√† c≈© sau s√°p nh·∫≠p")
    print("=" * 70)
    
    print("\nT√πy ch·ªçn:")
    print("1. Test nhanh (3 t·ªânh, 5 x√£/t·ªânh)")
    print("2. K√©o to√†n b·ªô d·ªØ li·ªáu (tu·∫ßn t·ª±)")
    print("3. K√©o to√†n b·ªô d·ªØ li·ªáu (ƒëa lu·ªìng - nhanh h∆°n)")
    
    try:
        choice = input("\nNh·∫≠p l·ª±a ch·ªçn (1-3): ").strip()
        
        if choice == "1":
            crawler = SapNhapCrawlerFinal(max_workers=1)
            data = crawler.crawl_sample_data()
            
        elif choice == "2":
            confirm = input("C·∫£nh b√°o: S·∫Ω k√©o TO√ÄN B·ªò d·ªØ li·ªáu (~60 t·ªânh). C√≥ th·ªÉ m·∫•t 2-3 gi·ªù. Ti·∫øp t·ª•c? (y/N): ").strip().lower()
            if confirm == 'y':
                crawler = SapNhapCrawlerFinal(max_workers=1)
                data = crawler.crawl_full_data(use_threading=False)
            else:
                print("ƒê√£ h·ªßy.")
                return
                
        elif choice == "3":
            confirm = input("C·∫£nh b√°o: S·∫Ω k√©o TO√ÄN B·ªò d·ªØ li·ªáu v·ªõi ƒëa lu·ªìng. C√≥ th·ªÉ g√¢y t·∫£i cho server. Ti·∫øp t·ª•c? (y/N): ").strip().lower()
            if confirm == 'y':
                max_workers = int(input("S·ªë lu·ªìng (1-5, khuy·∫øn ngh·ªã 3): ") or "3")
                crawler = SapNhapCrawlerFinal(max_workers=max(1, min(5, max_workers)))
                data = crawler.crawl_full_data(use_threading=True)
            else:
                print("ƒê√£ h·ªßy.")
                return
        else:
            print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá. Ch·∫°y test m·∫∑c ƒë·ªãnh.")
            crawler = SapNhapCrawlerFinal(max_workers=1)
            data = crawler.crawl_sample_data()
        
        # L∆∞u k·∫øt qu·∫£
        if data:
            filename = crawler.save_to_excel()
            
            print(f"\n=== K·∫æT QU·∫¢ ===")
            df = pd.DataFrame(data)
            
            # Th·ªëng k√™ chi ti·∫øt
            print(f"üìä T·ªïng s·ªë b·∫£n ghi: {len(df)}")
            
            if 'cap_hanh_chinh' in df.columns:
                print(f"\nüìã Ph√¢n lo·∫°i theo c·∫•p:")
                for cap, count in df['cap_hanh_chinh'].value_counts().items():
                    print(f"   {cap}: {count}")
            
            if 'ten_tinh' in df.columns:
                print(f"\nüèõÔ∏è  S·ªë t·ªânh/th√†nh: {df['ten_tinh'].nunique()}")
                
            if 'so_luong_thay_doi' in df.columns:
                total_changes = df['so_luong_thay_doi'].sum()
                print(f"üîÑ T·ªïng s·ªë thay ƒë·ªïi: {total_changes}")
            
            print(f"\nüíæ File Excel: {filename}")
            
            if crawler.failed_requests:
                print(f"‚ö†Ô∏è  S·ªë request l·ªói: {len(crawler.failed_requests)}")
            
            print(f"\n‚úÖ HO√ÄN TH√ÄNH!")
            
        else:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c k√©o v·ªÅ")
            
    except KeyboardInterrupt:
        print("\n\nüõë ƒê√£ d·ª´ng theo y√™u c·∫ßu ng∆∞·ªùi d√πng")
        if 'crawler' in locals() and crawler.data:
            print("üíæ ƒêang l∆∞u d·ªØ li·ªáu ƒë√£ k√©o ƒë∆∞·ª£c...")
            crawler.save_to_excel()
            
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()
        
        if 'crawler' in locals() and crawler.data:
            print("üíæ ƒêang l∆∞u d·ªØ li·ªáu ƒë√£ k√©o ƒë∆∞·ª£c...")
            crawler.save_to_excel()

if __name__ == "__main__":
    main()
